{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all modules\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "\n",
    "from Naive.node import Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937fa913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Code</th>\n",
       "      <th>W0</th>\n",
       "      <th>W1</th>\n",
       "      <th>W2</th>\n",
       "      <th>W3</th>\n",
       "      <th>W4</th>\n",
       "      <th>W5</th>\n",
       "      <th>W6</th>\n",
       "      <th>W7</th>\n",
       "      <th>W8</th>\n",
       "      <th>...</th>\n",
       "      <th>W42</th>\n",
       "      <th>W43</th>\n",
       "      <th>W44</th>\n",
       "      <th>W45</th>\n",
       "      <th>W46</th>\n",
       "      <th>W47</th>\n",
       "      <th>W48</th>\n",
       "      <th>W49</th>\n",
       "      <th>W50</th>\n",
       "      <th>W51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>P815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>P816</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>P817</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>P818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>P819</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Product_Code  W0  W1  W2  W3  W4  W5  W6  W7  W8  ...  W42  W43  W44  W45  \\\n",
       "0             P1  11  12  10   8  13  12  14  21   6  ...    4    7    8   10   \n",
       "1             P2   7   6   3   2   7   1   6   3   3  ...    2    4    5    1   \n",
       "2             P3   7  11   8   9  10   8   7  13  12  ...    6   14    5    5   \n",
       "3             P4  12   8  13   5   9   6   9  13  13  ...    9   10    3    4   \n",
       "4             P5   8   5  13  11   6   7   9  14   9  ...    7   11    7   12   \n",
       "..           ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...   \n",
       "806         P815   0   0   1   0   0   2   1   0   0  ...    0    1    1    0   \n",
       "807         P816   0   1   0   0   1   2   2   6   0  ...    3    3    4    2   \n",
       "808         P817   1   0   0   0   1   1   2   1   1  ...    2    0    0    2   \n",
       "809         P818   0   0   0   1   0   0   0   0   1  ...    0    0    0    1   \n",
       "810         P819   0   1   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "\n",
       "     W46  W47  W48  W49  W50  W51  \n",
       "0     12    3    7    6    5   10  \n",
       "1      1    4    5    1    6    0  \n",
       "2      7    8   14    8    8    7  \n",
       "3      6    8   14    8    7    8  \n",
       "4      6    6    5   11    8    9  \n",
       "..   ...  ...  ...  ...  ...  ...  \n",
       "806    0    1    0    0    2    0  \n",
       "807    4    5    5    5    6    5  \n",
       "808    2    0    0    0    4    3  \n",
       "809    1    0    0    0    2    0  \n",
       "810    0    0    0    0    0    1  \n",
       "\n",
       "[811 rows x 53 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_data = pd.read_csv('Dataset/Sales_Transaction_Dataset_Weekly_Final.csv')\n",
    "#time_series_data = pd.read_csv('Dataset/cgm.csv')\n",
    "time_series_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858e92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(time_series_data.columns)\n",
    "time_series_index = columns.pop(0)  # remove product code\n",
    "\n",
    "time_series_dict = dict()\n",
    "\n",
    "for index, row in time_series_data.iterrows():\n",
    "    time_series_dict[row[time_series_index]] = list(row[columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6ba881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree_phase(time_series_data, P_value, paa_value,max_level):\n",
    "    \n",
    "    good_leaf_nodes = list()\n",
    "    bad_leaf_nodes = list()\n",
    "\n",
    "    print(\"Create-tree phase: start node splitting\")\n",
    "    node = Node(level=1, group=time_series_data, paa_value=5)\n",
    "    node.start_splitting(P_value, max_level, good_leaf_nodes, bad_leaf_nodes) # using naive method node splitting \n",
    "    \n",
    "    return good_leaf_nodes, bad_leaf_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0aa589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycle_bad_leaves_phase(p_value,good_leaf_nodes, bad_leaf_nodes,paa_value):\n",
    "    \n",
    "    suppressed_nodes = list()\n",
    "    bad_leaves_node_dict = dict()\n",
    "        \n",
    "    for node in bad_leaf_nodes:\n",
    "        if node.level in bad_leaves_node_dict.keys():\n",
    "            bad_leaves_node_dict[node.level].append(node)\n",
    "        else:\n",
    "            bad_leaves_node_dict[node.level] = [node]\n",
    "\n",
    "    bad_leaf_nodes_size = sum([node.size for node in bad_leaf_nodes])\n",
    "    \n",
    "        \n",
    "    if bad_leaf_nodes_size >= p_value:\n",
    "        \n",
    "        current_level = max(bad_leaves_node_dict.keys())\n",
    "        \n",
    "        while bad_leaf_nodes_size >= p_value:\n",
    "            \n",
    "            if current_level in bad_leaves_node_dict.keys():\n",
    "                leave_merge_dict = dict()\n",
    "                keys_remove_list = list()\n",
    "                merge = False\n",
    "                \n",
    "                for current_level_node in bad_leaves_node_dict[current_level]:\n",
    "                    pattern_rep_node = current_level_node.pattern_representation\n",
    "                    if pattern_rep_node in leave_merge_dict.keys():\n",
    "                        merge = True\n",
    "                        leave_merge_dict[pattern_rep_node].append(current_level_node)\n",
    "                        if pattern_rep_node in keys_remove_list:\n",
    "                            keys_remove_list.remove(pattern_rep_node)\n",
    "                        else:\n",
    "                            leave_merge_dict[pattern_rep_node] = [current_level_node]\n",
    "                            keys_remove_list.append(pattern_rep_node)\n",
    "                    \n",
    "                    if merge:\n",
    "                        for k in keys_remove_list:\n",
    "                            del leave_merge_dict[k]\n",
    "\n",
    "                        for pr, node_list in leave_merge_dict.items():\n",
    "                            group = dict()\n",
    "                            for node in node_list:\n",
    "                                bad_leaves_node_dict[current_level].remove(node)\n",
    "                                group.update(node.group)\n",
    "                            if current_level > 1:\n",
    "                                level = current_level\n",
    "                            else:\n",
    "                                level = 1\n",
    "                            leaf_merge = Node(level=level, pattern_representation=pr,\n",
    "                                group=group, paa_value=paa_value)\n",
    "\n",
    "                            if leaf_merge.size >= p_value:\n",
    "                                leaf_merge.label = \"good-leaf\"\n",
    "                                good_leaf_nodes.append(leaf_merge)\n",
    "                                bad_leaf_nodes_size -= leaf_merge.size\n",
    "                            else: \n",
    "                                leaf_merge.label = \"bad-leaf\"\n",
    "                                bad_leaves_node_dict[current_level].append(leaf_merge)\n",
    "\n",
    "                new_level = current_level-1\n",
    "                for node in bad_leaves_node_dict[current_level]:\n",
    "                    if new_level > 1:\n",
    "                        values_group = list(node.group.values())\n",
    "                        data = np.array(values_group[0])\n",
    "                        data_znorm = znorm(data)\n",
    "                        data_paa = paa(data_znorm, paa_value)\n",
    "                        pr = ts_to_string(data_paa, cuts_for_asize(new_level))\n",
    "                    else:\n",
    "                        pr = \"a\"*paa_value\n",
    "                    node.level = new_level\n",
    "                    node.pattern_representation = pr\n",
    "\n",
    "                if current_level > 0:\n",
    "                    if new_level not in bad_leaves_node_dict.keys():\n",
    "                        bad_leaves_node_dict[new_level] = bad_leaves_node_dict.pop(current_level)\n",
    "                    else:\n",
    "                        bad_leaves_node_dict[new_level] = bad_leaves_node_dict[new_level] + bad_leaves_node_dict.pop(current_level) \n",
    "                    current_level -= 1\n",
    "                else:\n",
    "                    break \n",
    "\n",
    "        \n",
    "        remaining_bad_leaf_nodes = list(bad_leaves_node_dict.values())[0]\n",
    "        for node in remaining_bad_leaf_nodes:\n",
    "            suppressed_nodes.append(node)\n",
    "        print (suppressed_nodes)\n",
    "\n",
    "    return suppressed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116face7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_down_greedy_clustering(T_series, size, T_clustered,T_structure, label='o', T_max_vals=None, T_min_vals=None):\n",
    "    \n",
    "    if len(T_series) < 2*size:\n",
    "        T_clustered.append(T_series)\n",
    "        T_structure.append(label)\n",
    "        return\n",
    "\n",
    "    ids = list(T_series.keys())\n",
    "\n",
    "    group_ut = dict()\n",
    "    group_vt = dict()\n",
    "\n",
    "    t_seed = ids[random.randint(0, len(ids) - 1)] \n",
    "    group_ut[t_seed] = T_series[t_seed]\n",
    "\n",
    "    old_t = t_seed # Last visited record\n",
    "\n",
    "    # to avoid this row to end up in two different groups\n",
    "    del T_series[t_seed]\n",
    "    ids.remove(t_seed)\n",
    "\n",
    "\n",
    "    rounds = 6 if len(T_series) >= 6 else len(T_series)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        if rnd % 2 == 0:\n",
    "            source = group_ut\n",
    "            target = group_vt\n",
    "        else:\n",
    "            source = group_vt\n",
    "            target = group_ut\n",
    "            \n",
    "        r = tuple_with_max_value_loss(source[old_t], T_series, old_t)\n",
    "\n",
    "        target[r] = T_series[r]\n",
    "        old_t = r\n",
    "\n",
    "        # Update data structures\n",
    "        del T_series[r]\n",
    "        ids.remove(r)\n",
    "\n",
    "    \n",
    "    random.shuffle(ids) \n",
    "\n",
    "    for i in ids:\n",
    "        row = T_series[i]\n",
    "\n",
    "        group_ut_vals = list(group_ut.values())\n",
    "        group_vt_vals = list(group_vt.values())\n",
    "\n",
    "        group_ut_vals.append(row)\n",
    "        group_vt_vals.append(row)\n",
    "        \n",
    "        metric_ut = instant_value_loss(group_ut_vals)\n",
    "        metric_vt = instant_value_loss(group_vt_vals)\n",
    "\n",
    "        if metric_vt < metric_ut:\n",
    "            group_vt[i] = row\n",
    "            del group_ut_vals[-1]\n",
    "        else:\n",
    "            group_ut[i] = row\n",
    "            del group_vt_vals[-1]\n",
    "\n",
    "        del T_series[i]\n",
    "\n",
    "    \n",
    "    if len(group_ut) >= size:\n",
    "        top_down_greedy_clustering(group_ut, size, T_clustered, T_structure, label + 'a', T_max_vals, T_min_vals) \n",
    "        T_clustered.append(group_ut)\n",
    "        T_structure.append(label + 'a')\n",
    "\n",
    "    if len(group_vt) >= size:\n",
    "        top_down_greedy_clustering(group_vt, size, T_clustered, T_structure, label + 'b', T_max_vals, T_min_vals) \n",
    "    else:\n",
    "        T_clustered.append(group_vt)\n",
    "        T_structure.append(label + 'b')\n",
    "\n",
    "\n",
    "def tuple_with_max_value_loss(base, T_series, key):\n",
    "    max_value_loss = 0\n",
    "    best_tuple = None\n",
    "\n",
    "    for k in T_series.keys():\n",
    "        if k != key:\n",
    "            value_loss = instant_value_loss([base, T_series[k]])\n",
    "\n",
    "            if value_loss >= max_value_loss: \n",
    "                max_value_loss = value_loss\n",
    "                best_tuple = k\n",
    "\n",
    "    return best_tuple\n",
    "\n",
    "def instant_value_loss(T, r_plus=None, r_minus=None):\n",
    "  \n",
    "\n",
    "    T_len = len(T[0])  # # of QI attributes in T\n",
    "\n",
    "    if not r_plus or not r_minus:\n",
    "        r_plus  = list()\n",
    "        r_minus = list()\n",
    "\n",
    "\n",
    "        for i in range(T_len): \n",
    "            r_plus_i  = 0\n",
    "            r_minus_i = float('inf')\n",
    "\n",
    "            for row in T:\n",
    "                if row[i] > r_plus_i:\n",
    "                    r_plus_i = row[i]\n",
    "\n",
    "                if row[i] < r_minus_i:\n",
    "                    r_minus_i = row[i]\n",
    "\n",
    "            r_plus.append(r_plus_i) \n",
    "            r_minus.append(r_minus_i)\n",
    "    \n",
    "    # Compute VL(t)  then VL(T)\n",
    "    value_loss_t = 0\n",
    "\n",
    "    for i in range(T_len):\n",
    "        value_loss_t += pow((r_plus[i] - r_minus[i]), 2) / T_len\n",
    "\n",
    "    value_loss_T = len(T)*np.sqrt(value_loss_t)\n",
    "    return value_loss_T\n",
    "\n",
    "\n",
    "def top_down_greedy_clustering_postprocessing(size, T_clustered, T_structure,T_postprocessed, T_max_vals=None, \n",
    "                                              T_min_vals=None):\n",
    "    t_index_merged = list()      \n",
    "    t_groups_merged = list()    \n",
    "    t_structure_merged = list() \n",
    "\n",
    "  \n",
    "    for idx, bad_group in enumerate(T_clustered):\n",
    "        bad_g_size = len(bad_group)\n",
    "        if bad_g_size < size: \n",
    "            bad_group_vals = list(bad_group.values())\n",
    "           \n",
    "            label = T_structure[idx]\n",
    "\n",
    "            \n",
    "            Near_neighbor_index = -1\n",
    "            Near_neighbor_found = False\n",
    "            Near_neighbor_metric = float('inf')\n",
    "\n",
    "           \n",
    "            for other_idx, other_label in enumerate(T_structure):\n",
    "                \n",
    "                if label[:-1] == other_label[:-1]: \n",
    "                    if idx == other_idx:\n",
    "                        continue\n",
    "\n",
    "                   \n",
    "                    if other_idx not in t_index_merged:\n",
    "                        Near_neighbor_found = True\n",
    "                        Near_neighbor_index = other_idx\n",
    "                        break\n",
    "           \n",
    "\n",
    "            merge_with_other_group = False\n",
    "            if Near_neighbor_found:\n",
    "                group_nn = T_clustered[Near_neighbor_index]\n",
    "            elif Near_neighbor_index !=idx:\n",
    "                if idx - 1 > 0:\n",
    "                    Near_neighbor_index = idx - 1\n",
    "                elif idx + 1 < len(T_structure) - 1:\n",
    "                    Near_neighbor_index = idx + 1 \n",
    "                group_nn = T_clustered[Near_neighbor_index]\n",
    "                merge_with_other_group = True\n",
    "\n",
    "            if Near_neighbor_found or merge_with_other_group:\n",
    "                group_merged_nn = bad_group_vals\n",
    "\n",
    "                \n",
    "                group_merged_nn = group_merged_nn  + list(group_nn.values())\n",
    "                \n",
    "                Near_neighbor_metric = instant_value_loss(group_merged_nn)\n",
    "\n",
    "                \n",
    "                group_merged_nn = dict()\n",
    "                group_merged_nn.update(bad_group)\n",
    "                group_merged_nn.update(group_nn)\n",
    "\n",
    "           \n",
    "            metric_large_group = float('inf')\n",
    "            large_group_index = -1\n",
    "\n",
    "            for other_idx, other_group in enumerate(T_clustered):\n",
    "               \n",
    "                if len(other_group) >= 2*size - bad_g_size: \n",
    "                   \n",
    "                    if other_idx not in t_index_merged:\n",
    "                        group_merged_large_g = bad_group.copy()\n",
    "                        group_large_g_vals = list(group_merged_large_g.values())\n",
    "\n",
    "                       \n",
    "                        for j in range(size - bad_g_size): # size - |G|\n",
    "                            tmp_metric = float('inf')\n",
    "\n",
    "                            best_record = {}\n",
    "                            best_row = []\n",
    "\n",
    "                           \n",
    "                            for ridx, row in other_group.items():\n",
    "                                if ridx not in group_merged_large_g.keys():\n",
    "                                    \n",
    "                                    metric = instant_value_loss(group_large_g_vals + [ row ])\n",
    "\n",
    "                                    if metric < tmp_metric: # Update min metric\n",
    "                                        best_record = { ridx : row }\n",
    "                                        tmp_metric = metric\n",
    "                                        best_row = row\n",
    "            \n",
    "                            group_merged_large_g.update(best_record)\n",
    "                            group_large_g_vals.append(best_row)\n",
    "\n",
    "                      \n",
    "                        if tmp_metric < metric_large_group:\n",
    "                            metric_large_group = tmp_metric\n",
    "                            large_group_index = other_idx\n",
    "\n",
    "                          \n",
    "                            leftover_group_large_g = { k : val for (k, val)\n",
    "                                    in other_group.items()\n",
    "                                    if k not in group_merged_large_g.keys() }\n",
    "     \n",
    "            if Near_neighbor_metric < metric_large_group: \n",
    "                t_index_merged.append(Near_neighbor_index)\n",
    "                t_groups_merged.append(group_merged_nn)\n",
    "                t_structure_merged.append(label[:-1]) \n",
    "               \n",
    "            else:\n",
    "               \n",
    "                t_index_merged.append(large_group_index)\n",
    "                t_groups_merged.append(group_merged_large_g)\n",
    "                t_groups_merged.append(leftover_group_large_g)\n",
    "              \n",
    "                t_structure_merged.append('')\n",
    "\n",
    "           \n",
    "            t_index_merged.append(idx)\n",
    "\n",
    "   \n",
    "    T_clustered = [ group for (idx, group)\n",
    "            in enumerate(T_clustered)\n",
    "            if idx not in t_index_merged ]\n",
    "    T_clustered += t_groups_merged \n",
    "\n",
    "    T_structure = [ label for (idx, label)\n",
    "            in enumerate(T_structure)\n",
    "            if idx not in t_index_merged]\n",
    "    T_structure += t_structure_merged\n",
    "\n",
    "    T_postprocessed += T_clustered\n",
    "\n",
    "    # Check if there are any more bad groups\n",
    "    bad_groups_cnt = 0\n",
    "\n",
    "    for group in T_clustered:\n",
    "        if len(group) < size:\n",
    "            bad_groups_cnt +=1\n",
    "\n",
    "    if bad_groups_cnt > 0: \n",
    "        top_down_greedy_clustering_postprocessing(size, T_clustered, T_structure,T_postprocessed, T_max_vals, T_min_vals)\n",
    "        \n",
    "\n",
    "def group_min_value_loss(group_to_search=None, group_to_merge=dict(), index_ignored=list()):\n",
    "    min_p_group = {\"group\" : dict(), \"index\" : None, \"value_loss\" : float(\"inf\")} \n",
    "    for index, group in enumerate(group_to_search):\n",
    "        if index not in index_ignored: \n",
    "            value_loss = instant_value_loss(list(group.values()) + list(group_to_merge.values()))\n",
    "            if value_loss < min_p_group[\"value_loss\"]:\n",
    "                min_p_group[\"value_loss\"] = value_loss\n",
    "                min_p_group[\"group\"] = group\n",
    "                min_p_group[\"index\"] = index\n",
    "\n",
    "    return min_p_group[\"group\"], min_p_group[\"index\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2627624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_formation_phase(p_subgroups, p, k ):\n",
    "    \n",
    "    kgroup_list= list()\n",
    "    \n",
    "    P_group_list = list() \n",
    "    splitted_p_subgroups = list()\n",
    "    p_subgroup_split_indexes = list()\n",
    "    \n",
    "    print(\"Start group formation phase\")\n",
    "    \n",
    "    # Initialize P_group_list \n",
    "    for p_subgroup in p_subgroups: \n",
    "        P_group_list.append(p_subgroup)\n",
    "\n",
    "  \n",
    "    for p_subgroup_index, p_subgroup in enumerate(P_group_list): \n",
    "\n",
    "        if len(p_subgroup) >= 2*p:\n",
    "        \n",
    "            tree_clustering = list()\n",
    "            temp_splitted_p_subgroup = list()\n",
    "\n",
    "            p_subgroup_to_be_splitted = p_subgroup.copy()\n",
    "            top_down_greedy_clustering( p_subgroup_to_be_splitted, p, temp_splitted_p_subgroup, tree_clustering)\n",
    " \n",
    "            postprocessed_p_subgroups = list()\n",
    "            top_down_greedy_clustering_postprocessing(p,temp_splitted_p_subgroup,tree_clustering,postprocessed_p_subgroups) \n",
    "                                                            \n",
    "            splitted_p_subgroups += postprocessed_p_subgroups\n",
    "            p_subgroup_split_indexes.append(p_subgroup_index) \n",
    "    \n",
    "    P_group_list = [p_subgroup for (p_subgroup_index, p_subgroup) in enumerate(P_group_list) if p_subgroup_index not in p_subgroup_split_indexes]\n",
    "    P_group_list += splitted_p_subgroups\n",
    "    \n",
    "    p_subgroups_k_promoted_idxs = list() \n",
    "\n",
    "    for p_subgroup_index, p_subgroup in enumerate(P_group_list):\n",
    "        \n",
    "        if len(p_subgroup) >= k:\n",
    "            p_subgroups_k_promoted_idxs.append(p_subgroup_index)\n",
    "            kgroup_list.append(p_subgroup)\n",
    "\n",
    "    \n",
    "    P_group_list = [p_subgroup for (p_subgroup_index, p_subgroup) in enumerate(P_group_list) if p_subgroup_index not in p_subgroups_k_promoted_idxs]\n",
    "\n",
    "    p_subgroups_index_merged = list()\n",
    "    P_group_list_size= sum([len(p_subgroup) for p_subgroup in P_group_list])\n",
    "\n",
    "  \n",
    "    while P_group_list_size>= k:\n",
    "     \n",
    "        Group, group_index = group_min_value_loss(group_to_search=P_group_list,index_ignored=p_subgroups_index_merged)\n",
    "        p_subgroups_index_merged.append(group_index) \n",
    "        P_group_list_size-= len(Group)\n",
    "\n",
    "        while len(Group) < k:\n",
    "           \n",
    "            group_min, group_min_index = group_min_value_loss(P_group_list,Group,p_subgroups_index_merged)\n",
    "            p_subgroups_index_merged.append(group_min_index)\n",
    "            \n",
    "            Group.update(group_min) \n",
    "            P_group_list_size-= len(group_min)\n",
    "       \n",
    "        kgroup_list.append(Group) \n",
    "\n",
    "\n",
    "    p_subgroups_left = [p_subgroup for (p_subgroup_index, p_subgroup) in enumerate(P_group_list) if p_subgroup_index not in p_subgroups_index_merged]\n",
    "\n",
    "    # for each remaining p-subgroup\n",
    "    for p_subgroup in p_subgroups_left:\n",
    "       \n",
    "        k_group_remaining, k_group_remaining_idx = group_min_value_loss(kgroup_list,p_subgroup)   \n",
    "        kgroup_list.pop(k_group_remaining_idx)\n",
    "        \n",
    "        k_group_remaining.update(p_subgroup)\n",
    "        kgroup_list.append(k_group_remaining)\n",
    "        \n",
    "    print(\"End group formation phase\")\n",
    "    \n",
    "    return kgroup_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eda5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymized_data(pattern_Rep_data,anonymized_data,suppressed_data):\n",
    "    \n",
    "    final_data_anonymized= dict()\n",
    "    \n",
    "    for index in range(0, len(anonymized_data)): \n",
    "        \n",
    "        group = anonymized_data[index]\n",
    "        \n",
    "        max_value = np.amax(np.array(list(group.values())), 0)\n",
    "        min_value = np.amin(np.array(list(group.values())), 0)\n",
    "        \n",
    "        for key in group.keys():\n",
    "            \n",
    "            final_data_anonymized[key] = list()\n",
    "            value_row = list()\n",
    "            for column_index in range(0, len(max_value)):\n",
    "                value_row.append(\"[{}-{}]\".format(min_value[column_index], max_value[column_index]))\n",
    "            \n",
    "            \n",
    "            value_row.append(pattern_Rep_data[key]) \n",
    "            value_row.append(\"Group: {}\".format(index))\n",
    "\n",
    "            final_data_anonymized[key] = value_row\n",
    "           \n",
    "        \n",
    "    for index in range(0, len(suppressed_data)):\n",
    "        group = suppressed_data[index]\n",
    "        for key in group.keys():\n",
    "            value_row = [\" - \"]*len(group[key])\n",
    "            value_row.append(\" - \") # pattern rapresentation\n",
    "            value_row.append(\" - \") # group\n",
    "            final_data_anonymized[key] = value_row\n",
    "            \n",
    "    \n",
    "    df_from_dict=pd.DataFrame.from_dict(final_data_anonymized,orient='index')\n",
    "    \n",
    "    return df_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d29719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create-tree phase: start node splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 10:58:01.554 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.555 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.556 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 29\n",
      "2022-04-23 10:58:01.577 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.580 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.581 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:01.611 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.615 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.617 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:01.619 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:01.634 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:01.739 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.740 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.742 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:01.743 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:01.764 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:01.810 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.811 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.815 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:01.883 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.885 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.886 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:01.920 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.922 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.923 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:01.940 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.941 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.943 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:01.956 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:01.959 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:01.960 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:01.964 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:01.981 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 2\n",
      "2022-04-23 10:58:02.001 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.002 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.004 | INFO     | Naive.node:start_splitting:125 - Merge all bad nodes in a single node, and label it as good-leaf\n",
      "2022-04-23 10:58:02.006 | INFO     | Naive.node:start_splitting:136 - Split only tg_nodes 2\n",
      "2022-04-23 10:58:02.022 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.024 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.025 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.026 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.044 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:02.272 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.273 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.275 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:02.455 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.456 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.457 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.513 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.514 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.516 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.606 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.607 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.608 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:02.634 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.635 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.637 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.688 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.689 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.690 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.692 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.713 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:02.757 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.758 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.759 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:02.775 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.777 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.781 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.783 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.803 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:02.837 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.838 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.839 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.857 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.860 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 10:58:02.863 | INFO     | Naive.node:start_splitting:125 - Merge all bad nodes in a single node, and label it as good-leaf\n",
      "2022-04-23 10:58:02.865 | INFO     | Naive.node:start_splitting:136 - Split only tg_nodes 2\n",
      "2022-04-23 10:58:02.867 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.881 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:02.887 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.903 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:02.926 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.930 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.932 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:02.942 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:02.943 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:02.947 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:02.949 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.970 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:02.974 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:02.990 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.033 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.034 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.034 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:03.046 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.047 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.048 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.059 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.060 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.063 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.083 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.085 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.086 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.181 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.182 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.183 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 6\n",
      "2022-04-23 10:58:03.215 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.216 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.217 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.250 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.251 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.253 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.267 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.268 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.269 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.282 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.283 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.284 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.292 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.294 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.295 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.311 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.312 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.313 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.371 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.372 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.373 | INFO     | Naive.node:start_splitting:125 - Merge all bad nodes in a single node, and label it as good-leaf\n",
      "2022-04-23 10:58:03.374 | INFO     | Naive.node:start_splitting:136 - Split only tg_nodes 4\n",
      "2022-04-23 10:58:03.400 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.401 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.403 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.415 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.416 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.418 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.448 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.450 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.451 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.466 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.467 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.468 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.509 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.511 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.512 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:03.531 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.532 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.534 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.561 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.563 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.564 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.588 | INFO     | Naive.node:start_splitting:89 - N can be split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 10:58:03.589 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.590 | INFO     | Naive.node:start_splitting:125 - Merge all bad nodes in a single node, and label it as good-leaf\n",
      "2022-04-23 10:58:03.592 | INFO     | Naive.node:start_splitting:136 - Split only tg_nodes 4\n",
      "2022-04-23 10:58:03.594 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.605 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.609 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.621 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.626 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.638 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.642 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.654 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.717 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.718 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.719 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:03.778 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.779 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.781 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.797 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.798 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.800 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.820 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.821 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.822 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:03.823 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.836 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.849 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.850 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.851 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.861 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.862 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.863 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.865 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.885 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.890 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.897 | INFO     | Naive.node:maximize_level_node:234 - Can't split again, max level already reached\n",
      "2022-04-23 10:58:03.909 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.911 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.912 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:03.923 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.924 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.925 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.927 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:03.948 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:03.970 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.973 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.973 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:03.991 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:03.993 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:03.994 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:03.996 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.007 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.099 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.100 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.101 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:04.118 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.119 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.120 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.172 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.173 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.174 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.209 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.210 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.211 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.261 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.262 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.264 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 3\n",
      "2022-04-23 10:58:04.306 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.307 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.308 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.310 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.320 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.325 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.338 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.350 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.351 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 10:58:04.352 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:04.354 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.368 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.372 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.383 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.401 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.402 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.403 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:04.424 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.426 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.427 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.429 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.445 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.477 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.478 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.478 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 5\n",
      "2022-04-23 10:58:04.488 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.490 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.492 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.493 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.510 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.514 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.530 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 4\n",
      "2022-04-23 10:58:04.541 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.542 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.544 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.554 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.555 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.556 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.576 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.578 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.579 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-23 10:58:04.592 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.594 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.595 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.608 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.609 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.611 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-23 10:58:04.625 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-23 10:58:04.626 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-23 10:58:04.628 | INFO     | Naive.node:start_splitting:125 - Merge all bad nodes in a single node, and label it as good-leaf\n",
      "2022-04-23 10:58:04.629 | INFO     | Naive.node:start_splitting:136 - Split only tg_nodes 1\n",
      "2022-04-23 10:58:04.630 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.643 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 2\n",
      "2022-04-23 10:58:04.648 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.655 | INFO     | Naive.node:maximize_level_node:234 - Can't split again, max level already reached\n",
      "2022-04-23 10:58:04.656 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-23 10:58:04.662 | INFO     | Naive.node:maximize_level_node:234 - Can't split again, max level already reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good leaf nodes :78 Bad leaf nodes :9\n",
      "\n",
      " recyling bad leaves phases starts\n",
      "[<Naive.node.Node object at 0x000001A482D64BB0>, <Naive.node.Node object at 0x000001A482D648E0>, <Naive.node.Node object at 0x000001A482E0BFA0>, <Naive.node.Node object at 0x000001A482E0B5E0>, <Naive.node.Node object at 0x000001A482E102E0>, <Naive.node.Node object at 0x000001A482E107C0>, <Naive.node.Node object at 0x000001A482DF0DC0>, <Naive.node.Node object at 0x000001A482DF0FA0>, <Naive.node.Node object at 0x000001A482DF0940>]\n",
      "Start group formation phase\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15500/3975724460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mtime_series_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_series_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mKAPRA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaa_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_series_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15500/3975724460.py\u001b[0m in \u001b[0;36mKAPRA\u001b[1;34m(K_value, P_value, paa_value, max_level, time_series_data)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#group formation phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mk_group_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_formation_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP_group_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;31m#print (k_group_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15500/1781973155.py\u001b[0m in \u001b[0;36mgroup_formation_phase\u001b[1;34m(p_subgroups, p, k)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGroup\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mgroup_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_min_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_min_value_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP_group_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGroup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_subgroups_index_merged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mp_subgroups_index_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_min_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15500/936547245.py\u001b[0m in \u001b[0;36mgroup_min_value_loss\u001b[1;34m(group_to_search, group_to_merge, index_ignored)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgroup_min_value_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_to_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_to_merge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_ignored\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mmin_p_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"group\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"index\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"value_loss\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_to_search\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex_ignored\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mvalue_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstant_value_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_to_merge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def KAPRA(K_value, P_value, paa_value ,max_level,time_series_data):\n",
    "    \n",
    "    #Create tree phase\n",
    "    good_leaf_nodes, bad_leaf_nodes = create_tree_phase(time_series_data, P_value, paa_value,max_level)\n",
    "    print ('Good leaf nodes :' + str (len (good_leaf_nodes)) + ' Bad leaf nodes :' + str (len (bad_leaf_nodes) ) )\n",
    "    \n",
    "    #Recycle bad-leaves phase\n",
    "    if(len(bad_leaf_nodes) > 0):\n",
    "        print ('\\n recyling bad leaves phases starts')\n",
    "        suppressed_nodes = recycle_bad_leaves_phase(P_value, good_leaf_nodes, bad_leaf_nodes, paa_value)\n",
    "    \n",
    " \n",
    "    suppressed_group_list =list()\n",
    "    P_group_list =list()\n",
    "    \n",
    "    Pattern_Representation =dict()\n",
    "    \n",
    "    for node in suppressed_nodes:\n",
    "        suppressed_group_list.append(node.group)\n",
    "    \n",
    "    for node in good_leaf_nodes:\n",
    "        P_group_list.append(node.group)\n",
    "        pr = node.pattern_representation\n",
    "\n",
    "        for key in node.group:\n",
    "            Pattern_Representation[key] = pr\n",
    "    \n",
    "    \n",
    "    #group formation phase\n",
    "    k_group_list = group_formation_phase(P_group_list, P_value,K_value)\n",
    "    #print (k_group_list)\n",
    "\n",
    "    #Anonymize data\n",
    "    anonymized_list= anonymized_data(Pattern_Representation,k_group_list,suppressed_group_list)\n",
    "    \n",
    "    return anonymized_list\n",
    "\n",
    "K_value = 10\n",
    "P_value = 2\n",
    "paa_value = 5\n",
    "max_level = 4 \n",
    "time_series_data = time_series_dict\n",
    "\n",
    "KAPRA(K_value, P_value, paa_value,max_level, time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing metics ETA, Instant value loss \n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from saxpy.paa import paa\n",
    "from saxpy.znorm import znorm \n",
    "from saxpy.alphabet import cuts_for_asize\n",
    "from saxpy.strfunc import idx2letter\n",
    "from saxpy.sax import ts_to_string, sax_by_chunking\n",
    "\n",
    "def letter2idx(letter):    \n",
    "    return ord(letter) - 97\n",
    "\n",
    "def empirical_median(paa_idx, seed=23, size=1000000):\n",
    "   \n",
    "    paa_reco = np.zeros(paa_idx.shape)\n",
    "    \n",
    "    level = np.max(paa_idx) + 1\n",
    "\n",
    "    if level > 1:\n",
    "        \n",
    "        # Get corresponding breakpoints\n",
    "        breakpoints = cuts_for_asize(level)\n",
    "        \n",
    "        # Empirical probabilistic median\n",
    "        np.random.seed(seed)\n",
    "        pts = np.random.normal(size=size)\n",
    "        \n",
    "        \n",
    "        # Get interval endpoints [beta_lo; beta_up)\n",
    "        for i in range(len(paa_idx)):\n",
    "            \n",
    "            # due to how breakpoints are stored: [beta_0, ... , beta_{l-1}]\n",
    "            start_idx = paa_idx[i]\n",
    "            beta_lo = breakpoints[start_idx]\n",
    "            if start_idx < level-1:\n",
    "                beta_up = breakpoints[paa_idx[i]+1]\n",
    "            else:\n",
    "                beta_up = np.inf\n",
    "            \n",
    "            paa_reco[i] = np.median(pts[(pts >= beta_lo) & (pts < beta_up)])\n",
    "    \n",
    "    return paa_reco\n",
    "\n",
    "\n",
    "\n",
    "def pattern_loss(series,pr,paa_size):\n",
    "    \n",
    "    znorm_threshold=0.01\n",
    "    \n",
    "    series_norm = znorm(series, znorm_threshold)\n",
    "    feature_vector = paa(series_norm, paa_size)\n",
    "    \n",
    "    # Get breakpoint indexes\n",
    "    paa_index = np.array([letter2idx(x) for x in pr])\n",
    "    paa_reconstruct = empirical_median(paa_idx)\n",
    "     \n",
    "    \n",
    "    if (np.sum(feature_vector) > 0) & (np.sum(paa_reconstruct) > 0):\n",
    "        cd = cosine(feature_vector,paa_reconstruct)\n",
    "    \n",
    "    # If both vectors are zero vectors the distance is 0 \n",
    "    elif (np.sum(feature_vector) == 0) & (np.sum(paa_reconstruct) == 0):\n",
    "        cd = 0.\n",
    "        \n",
    "    else:\n",
    "        cd = 1.\n",
    "        \n",
    "    \n",
    "    return cd, feature_vector, p_star\n",
    "\n",
    "\n",
    "\n",
    "def measure_metrics(K_value, P_value, paa_value,data):\n",
    "    \n",
    "    max_level = 4\n",
    "    \n",
    "    # calculate estimated time taken \n",
    "    start_time = time.time()    \n",
    "    anonymize_list_data = KAPRA(K_value, P_value, paa_value,max_level, data)\n",
    "    clear_output()\n",
    "    end_time = time.time()\n",
    "    Estimated_time_taken = round(float(end_time - start_time), 3)\n",
    "    \n",
    "    print ('ETA :' + str(Estimated_time_taken))\n",
    "    \n",
    "    \n",
    "    #Calculating Instant value loss\n",
    "    anonym_Instant_value_loss = 0\n",
    "    anonymize_df = anonymize_list_data\n",
    "\n",
    "    anonymize_df = anonymize_df.iloc[: , :-2]\n",
    "    \n",
    "    attribute_list = [list(anonymize_df.iloc[i]) for i in range(len(anonymize_df))]  # Quasi-identifier attributes\n",
    "    first_row = list(anonymize_df.iloc[0])\n",
    "\n",
    "    r_plus  = [0 for _ in range(len(first_row))]\n",
    "    r_minus = [0 for _ in range(len(first_row))]\n",
    "        \n",
    "    for i in range(len(first_row)):  \n",
    "        re = first_row[i]\n",
    "        mn,mx = re[1:-1].split('-')\n",
    "        r_minus[i] = int(mn)\n",
    "        r_plus[i] = int(mx)\n",
    "\n",
    "    anonym_Instant_value_loss += instant_value_loss(attribute_list, r_plus=r_plus, r_minus=r_minus)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    #calculating Pattern loss\n",
    "    t_series = len(data)\n",
    "    p_loss = np.zeros((t_series,))\n",
    "    anonymize_df =anonymize_list_data.iloc[: , :-1]\n",
    "    \n",
    "    for idx, k in enumerate(data.keys()):\n",
    "        \n",
    "        if k in anonymize_df.keys():\n",
    "            \n",
    "            series = data[k]\n",
    "            pr = anonymize_df[k]\n",
    "            paa_size = len(pr)\n",
    "            \n",
    "            pl,_,_ = pattern_loss(series,pr,paa_size)\n",
    "            \n",
    "            pattern_loss[idx] = pl\n",
    "            \n",
    "            \n",
    "    pattern_loss_t = np.sum(p_loss) \n",
    "    \n",
    "    '''\n",
    "\n",
    "    return Estimated_time_taken,anonym_Instant_value_loss #,pattern_loss_t\n",
    "\n",
    "    \n",
    "\n",
    "K_value = {5,10,15,20,25}\n",
    "\n",
    "paa_value = 5\n",
    "time_series_data = time_series_dict  \n",
    "\n",
    "Varying_K =pd.DataFrame(columns=['K_value', 'P_value' ,'Estimated Time', 'Instant Value Loss'])\n",
    "Varying_P =pd.DataFrame(columns=['K_value', 'P_value', 'Estimated Time', 'Instant Value Loss'])\n",
    "\n",
    "for i in K_value :\n",
    "    K_value = i\n",
    "    P_value = 2\n",
    "    ETA, IVL= measure_metrics(K_value, P_value, paa_value,time_series_data) \n",
    "    df2 = pd.DataFrame([[i,P_value,ETA,IVL]], columns=['K_value', 'P_value', 'Estimated Time', 'Instant Value Loss'])\n",
    "    Varying_K=Varying_K.append(df2)\n",
    "        \n",
    "Varying_K =Varying_K.sort_values(by=['K_value'])\n",
    "print(Varying_K)\n",
    "\n",
    "Varying_K.plot(x='K_value', y='Estimated Time' ,figsize=(10,5), grid=True)\n",
    "Varying_K.plot(x='K_value', y='Instant Value Loss' ,figsize=(10,5), grid=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20630361",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_value = {2,3,4,5,6}\n",
    "for j in P_value :\n",
    "    K_value = 5\n",
    "    P_value = j\n",
    "    ETA, IVL= measure_metrics(K_value, P_value, paa_value,time_series_data) \n",
    "    df3 = pd.DataFrame([[K_value,j,ETA,IVL]], columns=['K_value', 'P_value', 'Estimated Time', 'Instant Value Loss'])\n",
    "    Varying_P=Varying_P.append(df3)\n",
    "        \n",
    "Varying_P =Varying_P.sort_values(by=['P_value'])\n",
    "print(Varying_P)\n",
    "\n",
    "Varying_P.plot(x='P_value', y='Estimated Time' ,figsize=(10,5), grid=True)\n",
    "Varying_P.plot(x='P_value', y='Instant Value Loss' ,figsize=(10,5), grid=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7ab3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
