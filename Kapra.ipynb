{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all modules\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "\n",
    "from Naive.node import Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937fa913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Rec1</th>\n",
       "      <th>Rec2</th>\n",
       "      <th>Rec3</th>\n",
       "      <th>Rec4</th>\n",
       "      <th>Rec5</th>\n",
       "      <th>Rec6</th>\n",
       "      <th>Rec7</th>\n",
       "      <th>Rec8</th>\n",
       "      <th>Rec9</th>\n",
       "      <th>...</th>\n",
       "      <th>Rec11</th>\n",
       "      <th>Rec12</th>\n",
       "      <th>Rec13</th>\n",
       "      <th>Rec14</th>\n",
       "      <th>Rec15</th>\n",
       "      <th>Rec16</th>\n",
       "      <th>Rec17</th>\n",
       "      <th>Rec18</th>\n",
       "      <th>Rec19</th>\n",
       "      <th>Rec20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>204</td>\n",
       "      <td>201</td>\n",
       "      <td>204</td>\n",
       "      <td>200</td>\n",
       "      <td>196</td>\n",
       "      <td>192</td>\n",
       "      <td>191</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>156</td>\n",
       "      <td>150</td>\n",
       "      <td>147</td>\n",
       "      <td>145</td>\n",
       "      <td>142</td>\n",
       "      <td>137</td>\n",
       "      <td>127</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>168</td>\n",
       "      <td>163</td>\n",
       "      <td>180</td>\n",
       "      <td>203</td>\n",
       "      <td>226</td>\n",
       "      <td>244</td>\n",
       "      <td>255</td>\n",
       "      <td>285</td>\n",
       "      <td>...</td>\n",
       "      <td>302</td>\n",
       "      <td>315</td>\n",
       "      <td>327</td>\n",
       "      <td>336</td>\n",
       "      <td>334</td>\n",
       "      <td>323</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>318</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>115</td>\n",
       "      <td>136</td>\n",
       "      <td>162</td>\n",
       "      <td>183</td>\n",
       "      <td>198</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>118</td>\n",
       "      <td>129</td>\n",
       "      <td>134</td>\n",
       "      <td>137</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>183</td>\n",
       "      <td>188</td>\n",
       "      <td>176</td>\n",
       "      <td>167</td>\n",
       "      <td>173</td>\n",
       "      <td>160</td>\n",
       "      <td>145</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>189</td>\n",
       "      <td>199</td>\n",
       "      <td>211</td>\n",
       "      <td>222</td>\n",
       "      <td>230</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>243</td>\n",
       "      <td>243</td>\n",
       "      <td>242</td>\n",
       "      <td>216</td>\n",
       "      <td>215</td>\n",
       "      <td>218</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>188</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>185</td>\n",
       "      <td>180</td>\n",
       "      <td>174</td>\n",
       "      <td>164</td>\n",
       "      <td>151</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>102</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>92</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>107</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>123</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>153</td>\n",
       "      <td>162</td>\n",
       "      <td>171</td>\n",
       "      <td>162</td>\n",
       "      <td>163</td>\n",
       "      <td>158</td>\n",
       "      <td>150</td>\n",
       "      <td>140</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>125</td>\n",
       "      <td>131</td>\n",
       "      <td>138</td>\n",
       "      <td>145</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>139</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>115</td>\n",
       "      <td>117</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>119</td>\n",
       "      <td>123</td>\n",
       "      <td>125</td>\n",
       "      <td>123</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>108</td>\n",
       "      <td>106</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "      <td>102</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>104</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>105</td>\n",
       "      <td>101</td>\n",
       "      <td>100</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "      <td>108</td>\n",
       "      <td>115</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>104</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>176</td>\n",
       "      <td>173</td>\n",
       "      <td>170</td>\n",
       "      <td>167</td>\n",
       "      <td>163</td>\n",
       "      <td>160</td>\n",
       "      <td>156</td>\n",
       "      <td>152</td>\n",
       "      <td>149</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>184</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>194</td>\n",
       "      <td>199</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>206</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>171</td>\n",
       "      <td>169</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>184</td>\n",
       "      <td>189</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>202</td>\n",
       "      <td>204</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>240</td>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>256</td>\n",
       "      <td>261</td>\n",
       "      <td>262</td>\n",
       "      <td>259</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>249</td>\n",
       "      <td>248</td>\n",
       "      <td>246</td>\n",
       "      <td>242</td>\n",
       "      <td>238</td>\n",
       "      <td>232</td>\n",
       "      <td>226</td>\n",
       "      <td>220</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PID  Rec1  Rec2  Rec3  Rec4  Rec5  Rec6  Rec7  Rec8  Rec9  ...  Rec11  \\\n",
       "0     1   194   204   201   204   200   196   192   191   186  ...    162   \n",
       "1     2   163   168   163   180   203   226   244   255   285  ...    302   \n",
       "2     3    52    52    48    52    55    62    67    58    66  ...     88   \n",
       "3     4   112   107   108   118   129   134   137   143   152  ...    174   \n",
       "4     5   170   177   181   189   199   211   222   230   232  ...    239   \n",
       "5     6   188   190   192   185   180   174   164   151   135  ...    122   \n",
       "6     7   109   107   111   114   114   114   116   116   120  ...    121   \n",
       "7     8   153   162   171   162   163   158   150   140   133  ...    122   \n",
       "8     9   115   117   116   116   119   123   125   123   120  ...    111   \n",
       "9    10   103   105   101   100   105   110   108   115   118  ...    109   \n",
       "10   11   179   177   178   179   180   182   183   183   181  ...    176   \n",
       "11   12   185   185   184   186   186   186   188   187   187  ...    186   \n",
       "12   13   177   177   176   175   174   172   172   171   169  ...    174   \n",
       "13   14   240   249   253   256   261   262   259   258   258  ...    253   \n",
       "\n",
       "    Rec12  Rec13  Rec14  Rec15  Rec16  Rec17  Rec18  Rec19  Rec20  \n",
       "0     162    156    150    147    145    142    137    127    126  \n",
       "1     315    327    336    334    323    321    321    318    308  \n",
       "2      92     98     99    115    136    162    183    198    219  \n",
       "3     179    183    188    176    167    173    160    145    132  \n",
       "4     243    243    242    216    215    218    219    219    219  \n",
       "5     118    115    112    107    102    100    101     92     87  \n",
       "6     122    122    122    123    123    124    123    120    120  \n",
       "7     120    125    131    138    145    139    142    139    131  \n",
       "8     108    106    100    103    102     99     98    104    103  \n",
       "9     106    103    104    101    101    101    102    104    108  \n",
       "10    173    170    167    163    160    156    152    149    140  \n",
       "11    187    189    190    194    199    204    203    206    209  \n",
       "12    179    184    189    195    199    202    204    207    209  \n",
       "13    249    248    246    242    238    232    226    220    214  \n",
       "\n",
       "[14 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time_series_data = pd.read_csv('Dataset/Sales_Transaction_Dataset_Weekly_Final.csv')\n",
    "time_series_data = pd.read_csv('Dataset/cgm.csv')\n",
    "time_series_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858e92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(time_series_data.columns)\n",
    "time_series_index = columns.pop(0)  # remove product code\n",
    "\n",
    "time_series_dict = dict()\n",
    "\n",
    "for index, row in time_series_data.iterrows():\n",
    "    time_series_dict[row[time_series_index]] = list(row[columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6ba881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree_phase(time_series_data, P_value, paa_value,max_level):\n",
    "    \n",
    "    good_leaf_nodes = list()\n",
    "    bad_leaf_nodes = list()\n",
    "\n",
    "    print(\"Create-tree phase: start node splitting\")\n",
    "    node = Node(level=1, group=time_series_data, paa_value=5)\n",
    "    node.start_splitting(P_value, max_level, good_leaf_nodes, bad_leaf_nodes) # using naive method node splitting \n",
    "    \n",
    "    return good_leaf_nodes, bad_leaf_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0aa589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recycle_bad_leaves_phase(p_value,good_leaf_nodes, bad_leaf_nodes,paa_value):\n",
    "    \n",
    "    suppressed_nodes = list()\n",
    "    bad_leaves_node_dict = dict()\n",
    "        \n",
    "    for node in bad_leaf_nodes:\n",
    "        if node.level in bad_leaves_node_dict.keys():\n",
    "            bad_leaves_node_dict[node.level].append(node)\n",
    "        else:\n",
    "            bad_leaves_node_dict[node.level] = [node]\n",
    "\n",
    "    bad_leaf_nodes_size = sum([node.size for node in bad_leaf_nodes])\n",
    "    \n",
    "        \n",
    "    if bad_leaf_nodes_size >= p_value:\n",
    "        \n",
    "        current_level = max(bad_leaves_node_dict.keys())\n",
    "        \n",
    "        while bad_leaf_nodes_size >= p_value:\n",
    "            \n",
    "            if current_level in bad_leaves_node_dict.keys():\n",
    "                leave_merge_dict = dict()\n",
    "                keys_remove_list = list()\n",
    "                merge = False\n",
    "                \n",
    "                for current_level_node in bad_leaves_node_dict[current_level]:\n",
    "                    pattern_rep_node = current_level_node.pattern_representation\n",
    "                    if pattern_rep_node in leave_merge_dict.keys():\n",
    "                        merge = True\n",
    "                        leave_merge_dict[pattern_rep_node].append(current_level_node)\n",
    "                        if pattern_rep_node in keys_remove_list:\n",
    "                            keys_remove_list.remove(pattern_rep_node)\n",
    "                        else:\n",
    "                            leave_merge_dict[pattern_rep_node] = [current_level_node]\n",
    "                            keys_remove_list.append(pattern_rep_node)\n",
    "                    \n",
    "                    if merge:\n",
    "                        for k in keys_remove_list:\n",
    "                            del leave_merge_dict[k]\n",
    "\n",
    "                        for pr, node_list in leave_merge_dict.items():\n",
    "                            group = dict()\n",
    "                            for node in node_list:\n",
    "                                bad_leaves_node_dict[current_level].remove(node)\n",
    "                                group.update(node.group)\n",
    "                            if current_level > 1:\n",
    "                                level = current_level\n",
    "                            else:\n",
    "                                level = 1\n",
    "                            leaf_merge = Node(level=level, pattern_representation=pr,\n",
    "                                group=group, paa_value=paa_value)\n",
    "\n",
    "                            if leaf_merge.size >= p_value:\n",
    "                                leaf_merge.label = \"good-leaf\"\n",
    "                                good_leaf_nodes.append(leaf_merge)\n",
    "                                bad_leaf_nodes_size -= leaf_merge.size\n",
    "                            else: \n",
    "                                leaf_merge.label = \"bad-leaf\"\n",
    "                                bad_leaves_node_dict[current_level].append(leaf_merge)\n",
    "\n",
    "                new_level = current_level-1\n",
    "                for node in bad_leaves_node_dict[current_level]:\n",
    "                    if new_level > 1:\n",
    "                        values_group = list(node.group.values())\n",
    "                        data = np.array(values_group[0])\n",
    "                        data_znorm = znorm(data)\n",
    "                        data_paa = paa(data_znorm, paa_value)\n",
    "                        pr = ts_to_string(data_paa, cuts_for_asize(new_level))\n",
    "                    else:\n",
    "                        pr = \"a\"*paa_value\n",
    "                    node.level = new_level\n",
    "                    node.pattern_representation = pr\n",
    "\n",
    "                if current_level > 0:\n",
    "                    if new_level not in bad_leaves_node_dict.keys():\n",
    "                        bad_leaves_node_dict[new_level] = bad_leaves_node_dict.pop(current_level)\n",
    "                    else:\n",
    "                        bad_leaves_node_dict[new_level] = bad_leaves_node_dict[new_level] + bad_leaves_node_dict.pop(current_level) \n",
    "                    current_level -= 1\n",
    "                else:\n",
    "                    break \n",
    "\n",
    "        \n",
    "        remaining_bad_leaf_nodes = list(bad_leaves_node_dict.values())[0]\n",
    "        for node in remaining_bad_leaf_nodes:\n",
    "            suppressed_nodes.append(node)\n",
    "        print (suppressed_nodes)\n",
    "\n",
    "    return suppressed_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116face7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_down_greedy_clustering(T, size, T_clustered,T_structure, label='o', T_max_vals=None, T_min_vals=None):\n",
    "    \n",
    "    if len(T) < 2*size:\n",
    "        T_clustered.append(T)\n",
    "        T_structure.append(label)\n",
    "        return\n",
    "\n",
    "    ids = list(T.keys())\n",
    "\n",
    "    # 1. Initialize groups via a NCP maximization-based heuristic\n",
    "    group_u = dict()\n",
    "    group_v = dict()\n",
    "\n",
    "    seed = ids[random.randint(0, len(ids) - 1)] # Draw a random row Id\n",
    "    group_u[seed] = T[seed]\n",
    "\n",
    "    old = seed # Last visited record\n",
    "\n",
    "    # to avoid this row to end up in two different groups\n",
    "    del T[seed]\n",
    "    ids.remove(seed)\n",
    "\n",
    "\n",
    "    rounds = 6 if len(T) >= 6 else len(T)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        if rnd % 2 == 0:\n",
    "            source = group_u\n",
    "            target = group_v\n",
    "        else:\n",
    "            source = group_v\n",
    "            target = group_u\n",
    "            \n",
    "        r = find_tuple_with_max_vl(source[old], T, old)\n",
    "\n",
    "        target[r] = T[r]\n",
    "        old = r\n",
    "\n",
    "        # Update data structures\n",
    "        del T[r]\n",
    "        ids.remove(r)\n",
    "\n",
    "    # 1.b Assign each record to the group with lower NCP\n",
    "    random.shuffle(ids) # Shuffle leftover Ids\n",
    "\n",
    "    for i in ids:\n",
    "        row = T[i]\n",
    "\n",
    "        # Copy values to check what would happen\n",
    "        # if row was added to either one separately\n",
    "        group_u_vals = list(group_u.values())\n",
    "        group_v_vals = list(group_v.values())\n",
    "\n",
    "        group_u_vals.append(row)\n",
    "        group_v_vals.append(row)\n",
    "        \n",
    "        metric_u = instant_value_loss(group_u_vals)\n",
    "        metric_v = instant_value_loss(group_v_vals)\n",
    "\n",
    "        if metric_v < metric_u:\n",
    "            group_v[i] = row\n",
    "            del group_u_vals[-1]\n",
    "        else:\n",
    "            group_u[i] = row\n",
    "            del group_v_vals[-1]\n",
    "\n",
    "        del T[i]\n",
    "\n",
    "    # 2. Iterate recursively, or store groups if base case\n",
    "    if len(group_u) >= size:\n",
    "        top_down_greedy_clustering(group_u, size, T_clustered, T_structure, label + 'a', T_max_vals, T_min_vals) \n",
    "        T_clustered.append(group_u)\n",
    "        T_structure.append(label + 'a')\n",
    "\n",
    "    if len(group_v) >= size:\n",
    "        top_down_greedy_clustering(group_v, size, T_clustered, T_structure, label + 'b', T_max_vals, T_min_vals) \n",
    "    else:\n",
    "        T_clustered.append(group_v)\n",
    "        T_structure.append(label + 'b')\n",
    "\n",
    "\n",
    "def find_tuple_with_max_vl(base, T, key):\n",
    "    max_vl = 0\n",
    "    best = None\n",
    "\n",
    "    for k in T.keys():\n",
    "        if k != key:\n",
    "            vl = instant_value_loss([base, T[k]])\n",
    "\n",
    "            if vl >= max_vl: # Update the best tuple Id\n",
    "                max_vl = vl\n",
    "                best = k\n",
    "\n",
    "    return best\n",
    "\n",
    "def instant_value_loss(T, r_plus=None, r_minus=None):\n",
    "  \n",
    "\n",
    "    n = len(T[0])  # # of QI attributes in T\n",
    "\n",
    "    if not r_plus or not r_minus:\n",
    "        r_plus  = list()\n",
    "        r_minus = list()\n",
    "\n",
    "\n",
    "        for i in range(n): \n",
    "            r_plus_i  = 0\n",
    "            r_minus_i = float('inf')\n",
    "\n",
    "            for row in T:\n",
    "                if row[i] > r_plus_i:\n",
    "                    r_plus_i = row[i]\n",
    "\n",
    "                if row[i] < r_minus_i:\n",
    "                    r_minus_i = row[i]\n",
    "\n",
    "            r_plus.append(r_plus_i) \n",
    "            r_minus.append(r_minus_i)\n",
    "    \n",
    "    # Compute VL(t) and then VL(T)\n",
    "    vl_t = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        vl_t += pow((r_plus[i] - r_minus[i]), 2) / n\n",
    "\n",
    "    vl_T = len(T)*np.sqrt(vl_t)\n",
    "    return vl_T\n",
    "\n",
    "\n",
    "def top_down_greedy_clustering_postprocessing(size, T_clustered, T_structure,T_postprocessed, T_max_vals=None, \n",
    "                                              T_min_vals=None):\n",
    "    idxs_merged = list()      \n",
    "    groups_merged = list()    \n",
    "    structure_merged = list() \n",
    "\n",
    "  \n",
    "    for idx, bad_group in enumerate(T_clustered):\n",
    "        bad_g_size = len(bad_group)\n",
    "        if bad_g_size < size: # For any bad group\n",
    "            bad_group_vals = list(bad_group.values())\n",
    "           \n",
    "            label = T_structure[idx]\n",
    "\n",
    "            \n",
    "            idx_nn = -1\n",
    "            found_nn = False\n",
    "            metric_nn = float('inf')\n",
    "\n",
    "           \n",
    "            for other_idx, other_label in enumerate(T_structure):\n",
    "                \n",
    "                if label[:-1] == other_label[:-1]: \n",
    "                    if idx == other_idx:\n",
    "                        continue\n",
    "\n",
    "                   \n",
    "                    if other_idx not in idxs_merged:\n",
    "                        found_nn = True\n",
    "                        idx_nn = other_idx\n",
    "                        break\n",
    "           \n",
    "\n",
    "            merge_with_other_group = False\n",
    "            if found_nn:\n",
    "                group_nn = T_clustered[idx_nn]\n",
    "            elif idx_nn !=idx:\n",
    "                if idx - 1 > 0:\n",
    "                    idx_nn = idx - 1\n",
    "                elif idx + 1 < len(T_structure) - 1:\n",
    "                    idx_nn = idx + 1 \n",
    "                group_nn = T_clustered[idx_nn]\n",
    "                merge_with_other_group = True\n",
    "\n",
    "            if found_nn or merge_with_other_group:\n",
    "                group_merged_nn = bad_group_vals\n",
    "\n",
    "                \n",
    "                group_merged_nn = group_merged_nn  + list(group_nn.values())\n",
    "                \n",
    "                metric_nn = instant_value_loss(group_merged_nn)\n",
    "\n",
    "                # Redefine group_merged_nn as dict\n",
    "                group_merged_nn = dict()\n",
    "                group_merged_nn.update(bad_group)\n",
    "                group_merged_nn.update(group_nn)\n",
    "\n",
    "            # Find the most appropriate large group (>= 2*size -|G|) - 2nd candidate group\n",
    "            metric_large_g = float('inf')\n",
    "            idx_large_g = -1\n",
    "\n",
    "            for other_idx, other_group in enumerate(T_clustered):\n",
    "               \n",
    "                if len(other_group) >= 2*size - bad_g_size: \n",
    "                   \n",
    "                    if other_idx not in idxs_merged:\n",
    "                        group_merged_large_g = bad_group.copy()\n",
    "                        group_large_g_vals = list(group_merged_large_g.values())\n",
    "\n",
    "                       \n",
    "                        for j in range(size - bad_g_size): # size - |G|\n",
    "                            tmp_metric = float('inf')\n",
    "\n",
    "                            best_record = {}\n",
    "                            best_row = []\n",
    "\n",
    "                           \n",
    "                            for ridx, row in other_group.items():\n",
    "                                if ridx not in group_merged_large_g.keys():\n",
    "                                    \n",
    "                                    metric = instant_value_loss(group_large_g_vals + [ row ])\n",
    "\n",
    "                                    if metric < tmp_metric: # Update min metric\n",
    "                                        best_record = { ridx : row }\n",
    "                                        tmp_metric = metric\n",
    "                                        best_row = row\n",
    "            \n",
    "                            group_merged_large_g.update(best_record)\n",
    "                            group_large_g_vals.append(best_row)\n",
    "\n",
    "                      \n",
    "                        if tmp_metric < metric_large_g:\n",
    "                            metric_large_g = tmp_metric\n",
    "                            idx_large_g = other_idx\n",
    "\n",
    "                          \n",
    "                            leftover_group_large_g = { k : val for (k, val)\n",
    "                                    in other_group.items()\n",
    "                                    if k not in group_merged_large_g.keys() }\n",
    "     \n",
    "            if metric_nn < metric_large_g: \n",
    "                idxs_merged.append(idx_nn)\n",
    "                groups_merged.append(group_merged_nn)\n",
    "                structure_merged.append(label[:-1]) \n",
    "               \n",
    "            else:\n",
    "               \n",
    "                idxs_merged.append(idx_large_g)\n",
    "                groups_merged.append(group_merged_large_g)\n",
    "                groups_merged.append(leftover_group_large_g)\n",
    "              \n",
    "                structure_merged.append('')\n",
    "\n",
    "           \n",
    "            idxs_merged.append(idx)\n",
    "\n",
    "   \n",
    "    T_clustered = [ group for (idx, group)\n",
    "            in enumerate(T_clustered)\n",
    "            if idx not in idxs_merged ]\n",
    "    T_clustered += groups_merged \n",
    "\n",
    "    T_structure = [ label for (idx, label)\n",
    "            in enumerate(T_structure)\n",
    "            if idx not in idxs_merged]\n",
    "    T_structure += structure_merged\n",
    "\n",
    "    T_postprocessed += T_clustered\n",
    "\n",
    "    # 3. Check if there are any more bad groups\n",
    "    bad_groups_cnt = 0\n",
    "\n",
    "    for group in T_clustered:\n",
    "        if len(group) < size:\n",
    "            bad_groups_cnt +=1\n",
    "\n",
    "    if bad_groups_cnt > 0: # Call recursively if any left\n",
    "        top_down_greedy_clustering_postprocessing(size, T_clustered, T_structure,T_postprocessed, T_max_vals, T_min_vals)\n",
    "        \n",
    "\n",
    "def group_min_value_loss(group_to_search=None, group_to_merge=dict(), index_ignored=list()):\n",
    "    min_p_group = {\"group\" : dict(), \"index\" : None, \"vl\" : float(\"inf\")} \n",
    "    for index, group in enumerate(group_to_search):\n",
    "        if index not in index_ignored: \n",
    "            vl = instant_value_loss(list(group.values()) + list(group_to_merge.values()))\n",
    "            if vl < min_p_group[\"vl\"]:\n",
    "                min_p_group[\"vl\"] = vl\n",
    "                min_p_group[\"group\"] = group\n",
    "                min_p_group[\"index\"] = index\n",
    "\n",
    "    return min_p_group[\"group\"], min_p_group[\"index\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2627624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_formation_phase(p_subgroups, p, k ):\n",
    "    \n",
    "    kgroup_list= list()\n",
    "    \n",
    "    P_group_list = list() \n",
    "    splitted_p_subgroups = list()\n",
    "    p_subgroup_split_indexes = list()\n",
    "    \n",
    "    print(\"Start group formation phase\")\n",
    "    \n",
    "    # Initialize P_group_list \n",
    "    for p_subgroup in p_subgroups: \n",
    "        P_group_list.append(p_subgroup)\n",
    "\n",
    "  \n",
    "    for p_subgroup_index, p_subgroup in enumerate(P_group_list): \n",
    "\n",
    "        if len(p_subgroup) >= 2*p:\n",
    "        \n",
    "            tree_clustering = list()\n",
    "            temp_splitted_p_subgroup = list()\n",
    "\n",
    "            p_subgroup_to_be_splitted = p_subgroup.copy()\n",
    "            top_down_greedy_clustering( p_subgroup_to_be_splitted, p, temp_splitted_p_subgroup, tree_clustering)\n",
    " \n",
    "            postprocessed_p_subgroups = list()\n",
    "            top_down_greedy_clustering_postprocessing(p,temp_splitted_p_subgroup,tree_clustering,postprocessed_p_subgroups) \n",
    "                                                            \n",
    "            splitted_p_subgroups += postprocessed_p_subgroups\n",
    "            p_subgroup_split_indexes.append(p_subgroup_index) \n",
    "    \n",
    "    P_group_list = [p_subgroup for (p_subgroup_index, p_subgroup) in enumerate(P_group_list) if p_subgroup_index not in p_subgroup_split_indexes]\n",
    "    P_group_list += splitted_p_subgroups\n",
    "    \n",
    "    p_subgroups_k_promoted_idxs = list() \n",
    "\n",
    "    for p_subgroup_index, p_subgroup in enumerate(P_group_list):\n",
    "        \n",
    "        if len(p_subgroup) >= k:\n",
    "            p_subgroups_k_promoted_idxs.append(p_subgroup_index)\n",
    "            kgroup_list.append(p_subgroup)\n",
    "\n",
    "    \n",
    "    P_group_list = [p_subgroup for (p_subgroup_index, p_subgroup) in enumerate(P_group_list) if p_subgroup_index not in p_subgroups_k_promoted_idxs]\n",
    "\n",
    "    p_subgroups_index_merged = list()\n",
    "    P_group_list_size= sum([len(p_subgroup) for p_subgroup in P_group_list])\n",
    "\n",
    "  \n",
    "    while P_group_list_size>= k:\n",
    "     \n",
    "        Group, group_index = group_min_value_loss(group_to_search=P_group_list,index_ignored=p_subgroups_index_merged)\n",
    "        p_subgroups_index_merged.append(group_index) \n",
    "        P_group_list_size-= len(Group)\n",
    "\n",
    "        while len(Group) < k:\n",
    "           \n",
    "            group_min, group_min_index = group_min_value_loss(P_group_list,Group,p_subgroups_index_merged)\n",
    "            p_subgroups_index_merged.append(group_min_index)\n",
    "            \n",
    "            Group.update(group_min) \n",
    "            P_group_list_size-= len(group_min)\n",
    "       \n",
    "        kgroup_list.append(Group) \n",
    "\n",
    "\n",
    "    p_subgroups_left = [p_subgroup for (p_subgroup_index, p_subgroup) in enumerate(P_group_list) if p_subgroup_index not in p_subgroups_index_merged]\n",
    "\n",
    "    # for each remaining p-subgroup\n",
    "    for p_subgroup in p_subgroups_left:\n",
    "       \n",
    "        k_group_remaining, k_group_remaining_idx = group_min_value_loss(kgroup_list,p_subgroup)   \n",
    "        kgroup_list.pop(k_group_remaining_idx)\n",
    "        \n",
    "        k_group_remaining.update(p_subgroup)\n",
    "        kgroup_list.append(k_group_remaining)\n",
    "        \n",
    "    print(\"End group formation phase\")\n",
    "    \n",
    "    return kgroup_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c263d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymized_data(pattern_Rep_data,anonymized_data,suppressed_data):\n",
    "    \n",
    "    final_data_anonymized= dict()\n",
    "    \n",
    "    for index in range(0, len(anonymized_data)): \n",
    "        \n",
    "        group = anonymized_data[index]\n",
    "        \n",
    "        max_value = np.amax(np.array(list(group.values())), 0)\n",
    "        min_value = np.amin(np.array(list(group.values())), 0)\n",
    "        \n",
    "        for key in group.keys():\n",
    "            \n",
    "            final_data_anonymized[key] = list()\n",
    "            value_row = list()\n",
    "            for column_index in range(0, len(max_value)):\n",
    "                value_row.append(\"[{}-{}]\".format(min_value[column_index], max_value[column_index]))\n",
    "            \n",
    "            \n",
    "            value_row.append(pattern_Rep_data[key]) \n",
    "            value_row.append(\"Group: {}\".format(index))\n",
    "\n",
    "            final_data_anonymized[key] = value_row\n",
    "           \n",
    "        \n",
    "    for index in range(0, len(suppressed_data)):\n",
    "        group = suppressed_data[index]\n",
    "        for key in group.keys():\n",
    "            value_row = [\" - \"]*len(group[key])\n",
    "            value_row.append(\" - \") # pattern rapresentation\n",
    "            value_row.append(\" - \") # group\n",
    "            final_data_anonymized[key] = value_row\n",
    "            \n",
    "    \n",
    "    your_df_from_dict=pd.DataFrame.from_dict(final_data_anonymized,orient='index')\n",
    "    print (your_df_from_dict)\n",
    "    \n",
    "    your_df_from_dict.to_csv('Dataset/out.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d29719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 17:46:51.303 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-22 17:46:51.306 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-22 17:46:51.307 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 4\n",
      "2022-04-22 17:46:51.309 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-22 17:46:51.315 | INFO     | Naive.node:maximize_level_node:234 - Can't split again, max level already reached\n",
      "2022-04-22 17:46:51.321 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-22 17:46:51.323 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-22 17:46:51.325 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-22 17:46:51.328 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-22 17:46:51.336 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 2\n",
      "2022-04-22 17:46:51.339 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-22 17:46:51.352 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create-tree phase: start node splitting\n",
      "Good leaf nodes :4 Bad leaf nodes :1\n",
      "\n",
      " recyling bad leaves phases starts\n",
      "Start group formation phase\n",
      "End group formation phase\n",
      "          0         1         2         3         4         5         6   \\\n",
      "4   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "7   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "1   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "6   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "8   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "3   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "12  [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "13  [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "9   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "11  [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "14  [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "2   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "5   [52-240]  [52-249]  [48-253]  [52-256]  [55-261]  [62-262]  [67-259]   \n",
      "\n",
      "          7         8         9   ...        12        13         14  \\\n",
      "4   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "7   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "1   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "6   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "8   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "3   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "12  [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "13  [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "9   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "11  [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "14  [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "2   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "5   [58-258]  [66-285]  [77-287]  ...  [98-327]  [99-336]  [103-334]   \n",
      "\n",
      "           15        16        17        18        19     20        21  \n",
      "4   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    abb  Group: 0  \n",
      "7   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    abb  Group: 0  \n",
      "1   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]  bbaaa  Group: 0  \n",
      "6   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]  bbaaa  Group: 0  \n",
      "8   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]  bbaaa  Group: 0  \n",
      "3   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    aab  Group: 0  \n",
      "12  [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    aab  Group: 0  \n",
      "13  [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    aab  Group: 0  \n",
      "9   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    bba  Group: 0  \n",
      "11  [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    bba  Group: 0  \n",
      "14  [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    bba  Group: 0  \n",
      "2   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    abb  Group: 0  \n",
      "5   [102-323]  [99-321]  [98-321]  [92-318]  [87-308]    abb  Group: 0  \n",
      "\n",
      "[13 rows x 22 columns]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Dataset/out.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23004/919303966.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mtime_series_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_series_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mKAPRA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaa_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_series_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23004/919303966.py\u001b[0m in \u001b[0;36mKAPRA\u001b[1;34m(K_value, P_value, paa_value, max_level, time_series_data)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#Anonymize data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0manonymized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPattern_Representation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk_group_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msuppressed_group_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23004/3664317977.py\u001b[0m in \u001b[0;36manonymized_data\u001b[1;34m(pattern_Rep_data, anonymized_data, suppressed_data)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0myour_df_from_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0myour_df_from_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dataset/out.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Dataset/out.csv'"
     ]
    }
   ],
   "source": [
    "def KAPRA(K_value, P_value, paa_value ,max_level,time_series_data):\n",
    "    \n",
    "    #Create tree phase\n",
    "    good_leaf_nodes, bad_leaf_nodes = create_tree_phase(time_series_data, P_value, paa_value,max_level)\n",
    "    print ('Good leaf nodes :' + str (len (good_leaf_nodes)) + ' Bad leaf nodes :' + str (len (bad_leaf_nodes) ) )\n",
    "    \n",
    "    #Recycle bad-leaves phase\n",
    "    if(len(bad_leaf_nodes) > 0):\n",
    "        print ('\\n recyling bad leaves phases starts')\n",
    "        suppressed_nodes = recycle_bad_leaves_phase(P_value, good_leaf_nodes, bad_leaf_nodes, paa_value)\n",
    "    \n",
    " \n",
    "    suppressed_group_list =list()\n",
    "    P_group_list =list()\n",
    "    \n",
    "    Pattern_Representation =dict()\n",
    "    \n",
    "    for node in suppressed_nodes:\n",
    "        suppressed_group_list.append(node.group)\n",
    "    \n",
    "    for node in good_leaf_nodes:\n",
    "        P_group_list.append(node.group)\n",
    "        pr = node.pattern_representation\n",
    "\n",
    "        for key in node.group:\n",
    "            Pattern_Representation[key] = pr\n",
    "    \n",
    "    \n",
    "    #group formation phase\n",
    "    k_group_list = group_formation_phase(P_group_list, P_value,K_value)\n",
    "    #print (k_group_list)\n",
    "\n",
    "    #Anonymize data\n",
    "    anonymized_data(Pattern_Representation,k_group_list,suppressed_group_list)\n",
    "    \n",
    "\n",
    "K_value = 10\n",
    "P_value = 2\n",
    "paa_value = 5\n",
    "max_level = 4 \n",
    "time_series_data = time_series_dict\n",
    "\n",
    "KAPRA(K_value, P_value, paa_value,max_level, time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Instant value loss and pattern loss \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
