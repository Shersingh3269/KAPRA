{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cf5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru in d:\\anaconda\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in d:\\anaconda\\lib\\site-packages (from loguru) (1.1.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in d:\\anaconda\\lib\\site-packages (from loguru) (0.4.4)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.16 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: pathlib2 in d:\\anaconda\\lib\\site-packages (2.3.6)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from pathlib2) (1.16.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: saxpy in d:\\anaconda\\lib\\site-packages (1.0.1.dev167)\n",
      "Requirement already satisfied: codecov in d:\\anaconda\\lib\\site-packages (from saxpy) (2.1.12)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from saxpy) (1.20.3)\n",
      "Requirement already satisfied: pytest in d:\\anaconda\\lib\\site-packages (from saxpy) (6.2.4)\n",
      "Requirement already satisfied: pytest-cov in d:\\anaconda\\lib\\site-packages (from saxpy) (3.0.0)\n",
      "Requirement already satisfied: coverage in d:\\anaconda\\lib\\site-packages (from codecov->saxpy) (6.3.2)\n",
      "Requirement already satisfied: requests>=2.7.9 in d:\\anaconda\\lib\\site-packages (from codecov->saxpy) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.9->codecov->saxpy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.9->codecov->saxpy) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.9->codecov->saxpy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.7.9->codecov->saxpy) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (1.1.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (21.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (1.10.0)\n",
      "Requirement already satisfied: toml in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (0.10.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (1.4.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from pytest->saxpy) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging->pytest->saxpy) (3.0.4)\n",
      "Requirement already satisfied: tomli in d:\\anaconda\\lib\\site-packages (from coverage->codecov->saxpy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Installing Required modules \n",
    "!pip install loguru\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pathlib2\n",
    "!pip install pandas\n",
    "!pip install saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30c4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all modules\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "\n",
    "from Naive.node import Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd967b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Rec1</th>\n",
       "      <th>Rec2</th>\n",
       "      <th>Rec3</th>\n",
       "      <th>Rec4</th>\n",
       "      <th>Rec5</th>\n",
       "      <th>Rec6</th>\n",
       "      <th>Rec7</th>\n",
       "      <th>Rec8</th>\n",
       "      <th>Rec9</th>\n",
       "      <th>...</th>\n",
       "      <th>Rec11</th>\n",
       "      <th>Rec12</th>\n",
       "      <th>Rec13</th>\n",
       "      <th>Rec14</th>\n",
       "      <th>Rec15</th>\n",
       "      <th>Rec16</th>\n",
       "      <th>Rec17</th>\n",
       "      <th>Rec18</th>\n",
       "      <th>Rec19</th>\n",
       "      <th>Rec20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>204</td>\n",
       "      <td>201</td>\n",
       "      <td>204</td>\n",
       "      <td>200</td>\n",
       "      <td>196</td>\n",
       "      <td>192</td>\n",
       "      <td>191</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>156</td>\n",
       "      <td>150</td>\n",
       "      <td>147</td>\n",
       "      <td>145</td>\n",
       "      <td>142</td>\n",
       "      <td>137</td>\n",
       "      <td>127</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>168</td>\n",
       "      <td>163</td>\n",
       "      <td>180</td>\n",
       "      <td>203</td>\n",
       "      <td>226</td>\n",
       "      <td>244</td>\n",
       "      <td>255</td>\n",
       "      <td>285</td>\n",
       "      <td>...</td>\n",
       "      <td>302</td>\n",
       "      <td>315</td>\n",
       "      <td>327</td>\n",
       "      <td>336</td>\n",
       "      <td>334</td>\n",
       "      <td>323</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>318</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "      <td>58</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>115</td>\n",
       "      <td>136</td>\n",
       "      <td>162</td>\n",
       "      <td>183</td>\n",
       "      <td>198</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>118</td>\n",
       "      <td>129</td>\n",
       "      <td>134</td>\n",
       "      <td>137</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>183</td>\n",
       "      <td>188</td>\n",
       "      <td>176</td>\n",
       "      <td>167</td>\n",
       "      <td>173</td>\n",
       "      <td>160</td>\n",
       "      <td>145</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>177</td>\n",
       "      <td>181</td>\n",
       "      <td>189</td>\n",
       "      <td>199</td>\n",
       "      <td>211</td>\n",
       "      <td>222</td>\n",
       "      <td>230</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>243</td>\n",
       "      <td>243</td>\n",
       "      <td>242</td>\n",
       "      <td>216</td>\n",
       "      <td>215</td>\n",
       "      <td>218</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>188</td>\n",
       "      <td>190</td>\n",
       "      <td>192</td>\n",
       "      <td>185</td>\n",
       "      <td>180</td>\n",
       "      <td>174</td>\n",
       "      <td>164</td>\n",
       "      <td>151</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>102</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>92</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>107</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>123</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>153</td>\n",
       "      <td>162</td>\n",
       "      <td>171</td>\n",
       "      <td>162</td>\n",
       "      <td>163</td>\n",
       "      <td>158</td>\n",
       "      <td>150</td>\n",
       "      <td>140</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>125</td>\n",
       "      <td>131</td>\n",
       "      <td>138</td>\n",
       "      <td>145</td>\n",
       "      <td>139</td>\n",
       "      <td>142</td>\n",
       "      <td>139</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>115</td>\n",
       "      <td>117</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>119</td>\n",
       "      <td>123</td>\n",
       "      <td>125</td>\n",
       "      <td>123</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>108</td>\n",
       "      <td>106</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "      <td>102</td>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>104</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>105</td>\n",
       "      <td>101</td>\n",
       "      <td>100</td>\n",
       "      <td>105</td>\n",
       "      <td>110</td>\n",
       "      <td>108</td>\n",
       "      <td>115</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>104</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>176</td>\n",
       "      <td>173</td>\n",
       "      <td>170</td>\n",
       "      <td>167</td>\n",
       "      <td>163</td>\n",
       "      <td>160</td>\n",
       "      <td>156</td>\n",
       "      <td>152</td>\n",
       "      <td>149</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>184</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>194</td>\n",
       "      <td>199</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>206</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>174</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>171</td>\n",
       "      <td>169</td>\n",
       "      <td>...</td>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>184</td>\n",
       "      <td>189</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>202</td>\n",
       "      <td>204</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>240</td>\n",
       "      <td>249</td>\n",
       "      <td>253</td>\n",
       "      <td>256</td>\n",
       "      <td>261</td>\n",
       "      <td>262</td>\n",
       "      <td>259</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>...</td>\n",
       "      <td>253</td>\n",
       "      <td>249</td>\n",
       "      <td>248</td>\n",
       "      <td>246</td>\n",
       "      <td>242</td>\n",
       "      <td>238</td>\n",
       "      <td>232</td>\n",
       "      <td>226</td>\n",
       "      <td>220</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PID  Rec1  Rec2  Rec3  Rec4  Rec5  Rec6  Rec7  Rec8  Rec9  ...  Rec11  \\\n",
       "0     1   194   204   201   204   200   196   192   191   186  ...    162   \n",
       "1     2   163   168   163   180   203   226   244   255   285  ...    302   \n",
       "2     3    52    52    48    52    55    62    67    58    66  ...     88   \n",
       "3     4   112   107   108   118   129   134   137   143   152  ...    174   \n",
       "4     5   170   177   181   189   199   211   222   230   232  ...    239   \n",
       "5     6   188   190   192   185   180   174   164   151   135  ...    122   \n",
       "6     7   109   107   111   114   114   114   116   116   120  ...    121   \n",
       "7     8   153   162   171   162   163   158   150   140   133  ...    122   \n",
       "8     9   115   117   116   116   119   123   125   123   120  ...    111   \n",
       "9    10   103   105   101   100   105   110   108   115   118  ...    109   \n",
       "10   11   179   177   178   179   180   182   183   183   181  ...    176   \n",
       "11   12   185   185   184   186   186   186   188   187   187  ...    186   \n",
       "12   13   177   177   176   175   174   172   172   171   169  ...    174   \n",
       "13   14   240   249   253   256   261   262   259   258   258  ...    253   \n",
       "\n",
       "    Rec12  Rec13  Rec14  Rec15  Rec16  Rec17  Rec18  Rec19  Rec20  \n",
       "0     162    156    150    147    145    142    137    127    126  \n",
       "1     315    327    336    334    323    321    321    318    308  \n",
       "2      92     98     99    115    136    162    183    198    219  \n",
       "3     179    183    188    176    167    173    160    145    132  \n",
       "4     243    243    242    216    215    218    219    219    219  \n",
       "5     118    115    112    107    102    100    101     92     87  \n",
       "6     122    122    122    123    123    124    123    120    120  \n",
       "7     120    125    131    138    145    139    142    139    131  \n",
       "8     108    106    100    103    102     99     98    104    103  \n",
       "9     106    103    104    101    101    101    102    104    108  \n",
       "10    173    170    167    163    160    156    152    149    140  \n",
       "11    187    189    190    194    199    204    203    206    209  \n",
       "12    179    184    189    195    199    202    204    207    209  \n",
       "13    249    248    246    242    238    232    226    220    214  \n",
       "\n",
       "[14 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Datasets CGM Data set\n",
    "\n",
    "time_series_data = pd.read_csv('Dataset/cgm.csv')\n",
    "time_series_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e2d603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rec1',\n",
       " 'Rec2',\n",
       " 'Rec3',\n",
       " 'Rec4',\n",
       " 'Rec5',\n",
       " 'Rec6',\n",
       " 'Rec7',\n",
       " 'Rec8',\n",
       " 'Rec9',\n",
       " 'Rec10',\n",
       " 'Rec11',\n",
       " 'Rec12',\n",
       " 'Rec13',\n",
       " 'Rec14',\n",
       " 'Rec15',\n",
       " 'Rec16',\n",
       " 'Rec17',\n",
       " 'Rec18',\n",
       " 'Rec19',\n",
       " 'Rec20']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(time_series_data.columns)\n",
    "time_series_index = columns.pop(0)  # remove product code\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6433798a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [194,\n",
       "  204,\n",
       "  201,\n",
       "  204,\n",
       "  200,\n",
       "  196,\n",
       "  192,\n",
       "  191,\n",
       "  186,\n",
       "  168,\n",
       "  162,\n",
       "  162,\n",
       "  156,\n",
       "  150,\n",
       "  147,\n",
       "  145,\n",
       "  142,\n",
       "  137,\n",
       "  127,\n",
       "  126],\n",
       " 2: [163,\n",
       "  168,\n",
       "  163,\n",
       "  180,\n",
       "  203,\n",
       "  226,\n",
       "  244,\n",
       "  255,\n",
       "  285,\n",
       "  287,\n",
       "  302,\n",
       "  315,\n",
       "  327,\n",
       "  336,\n",
       "  334,\n",
       "  323,\n",
       "  321,\n",
       "  321,\n",
       "  318,\n",
       "  308],\n",
       " 3: [52,\n",
       "  52,\n",
       "  48,\n",
       "  52,\n",
       "  55,\n",
       "  62,\n",
       "  67,\n",
       "  58,\n",
       "  66,\n",
       "  77,\n",
       "  88,\n",
       "  92,\n",
       "  98,\n",
       "  99,\n",
       "  115,\n",
       "  136,\n",
       "  162,\n",
       "  183,\n",
       "  198,\n",
       "  219],\n",
       " 4: [112,\n",
       "  107,\n",
       "  108,\n",
       "  118,\n",
       "  129,\n",
       "  134,\n",
       "  137,\n",
       "  143,\n",
       "  152,\n",
       "  164,\n",
       "  174,\n",
       "  179,\n",
       "  183,\n",
       "  188,\n",
       "  176,\n",
       "  167,\n",
       "  173,\n",
       "  160,\n",
       "  145,\n",
       "  132],\n",
       " 5: [170,\n",
       "  177,\n",
       "  181,\n",
       "  189,\n",
       "  199,\n",
       "  211,\n",
       "  222,\n",
       "  230,\n",
       "  232,\n",
       "  234,\n",
       "  239,\n",
       "  243,\n",
       "  243,\n",
       "  242,\n",
       "  216,\n",
       "  215,\n",
       "  218,\n",
       "  219,\n",
       "  219,\n",
       "  219],\n",
       " 6: [188,\n",
       "  190,\n",
       "  192,\n",
       "  185,\n",
       "  180,\n",
       "  174,\n",
       "  164,\n",
       "  151,\n",
       "  135,\n",
       "  126,\n",
       "  122,\n",
       "  118,\n",
       "  115,\n",
       "  112,\n",
       "  107,\n",
       "  102,\n",
       "  100,\n",
       "  101,\n",
       "  92,\n",
       "  87],\n",
       " 7: [109,\n",
       "  107,\n",
       "  111,\n",
       "  114,\n",
       "  114,\n",
       "  114,\n",
       "  116,\n",
       "  116,\n",
       "  120,\n",
       "  121,\n",
       "  121,\n",
       "  122,\n",
       "  122,\n",
       "  122,\n",
       "  123,\n",
       "  123,\n",
       "  124,\n",
       "  123,\n",
       "  120,\n",
       "  120],\n",
       " 8: [153,\n",
       "  162,\n",
       "  171,\n",
       "  162,\n",
       "  163,\n",
       "  158,\n",
       "  150,\n",
       "  140,\n",
       "  133,\n",
       "  128,\n",
       "  122,\n",
       "  120,\n",
       "  125,\n",
       "  131,\n",
       "  138,\n",
       "  145,\n",
       "  139,\n",
       "  142,\n",
       "  139,\n",
       "  131],\n",
       " 9: [115,\n",
       "  117,\n",
       "  116,\n",
       "  116,\n",
       "  119,\n",
       "  123,\n",
       "  125,\n",
       "  123,\n",
       "  120,\n",
       "  116,\n",
       "  111,\n",
       "  108,\n",
       "  106,\n",
       "  100,\n",
       "  103,\n",
       "  102,\n",
       "  99,\n",
       "  98,\n",
       "  104,\n",
       "  103],\n",
       " 10: [103,\n",
       "  105,\n",
       "  101,\n",
       "  100,\n",
       "  105,\n",
       "  110,\n",
       "  108,\n",
       "  115,\n",
       "  118,\n",
       "  113,\n",
       "  109,\n",
       "  106,\n",
       "  103,\n",
       "  104,\n",
       "  101,\n",
       "  101,\n",
       "  101,\n",
       "  102,\n",
       "  104,\n",
       "  108],\n",
       " 11: [179,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  182,\n",
       "  183,\n",
       "  183,\n",
       "  181,\n",
       "  179,\n",
       "  176,\n",
       "  173,\n",
       "  170,\n",
       "  167,\n",
       "  163,\n",
       "  160,\n",
       "  156,\n",
       "  152,\n",
       "  149,\n",
       "  140],\n",
       " 12: [185,\n",
       "  185,\n",
       "  184,\n",
       "  186,\n",
       "  186,\n",
       "  186,\n",
       "  188,\n",
       "  187,\n",
       "  187,\n",
       "  187,\n",
       "  186,\n",
       "  187,\n",
       "  189,\n",
       "  190,\n",
       "  194,\n",
       "  199,\n",
       "  204,\n",
       "  203,\n",
       "  206,\n",
       "  209],\n",
       " 13: [177,\n",
       "  177,\n",
       "  176,\n",
       "  175,\n",
       "  174,\n",
       "  172,\n",
       "  172,\n",
       "  171,\n",
       "  169,\n",
       "  171,\n",
       "  174,\n",
       "  179,\n",
       "  184,\n",
       "  189,\n",
       "  195,\n",
       "  199,\n",
       "  202,\n",
       "  204,\n",
       "  207,\n",
       "  209],\n",
       " 14: [240,\n",
       "  249,\n",
       "  253,\n",
       "  256,\n",
       "  261,\n",
       "  262,\n",
       "  259,\n",
       "  258,\n",
       "  258,\n",
       "  256,\n",
       "  253,\n",
       "  249,\n",
       "  248,\n",
       "  246,\n",
       "  242,\n",
       "  238,\n",
       "  232,\n",
       "  226,\n",
       "  220,\n",
       "  214]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_dict = dict()\n",
    "\n",
    "for index, row in time_series_data.iterrows():\n",
    "  time_series_dict[row[time_series_index]] = list(row[columns])\n",
    "\n",
    "time_series_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046cfa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 21:57:41.492 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-21 21:57:41.494 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-21 21:57:41.496 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-21 21:57:41.505 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-21 21:57:41.507 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-21 21:57:41.510 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 1 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-21 21:57:41.517 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-21 21:57:41.518 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-21 21:57:41.520 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-21 21:57:41.521 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:2, p_value:2 == good-leaf\n",
      "2022-04-21 21:57:41.527 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 2\n",
      "2022-04-21 21:57:41.537 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-21 21:57:41.539 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-21 21:57:41.541 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 2\n",
      "2022-04-21 21:57:41.548 | INFO     | Naive.node:start_splitting:89 - N can be split\n",
      "2022-04-21 21:57:41.550 | INFO     | Naive.node:start_splitting:90 - Compute tentative good nodes and tentative bad nodes\n",
      "2022-04-21 21:57:41.552 | INFO     | Naive.node:start_splitting:152 - Label all tb_node 0 as bad-leaf and split only tg_nodes 1\n",
      "2022-04-21 21:57:41.554 | INFO     | Naive.node:start_splitting:51 - Maximize-level, size:3, p_value:2 == good-leaf\n",
      "2022-04-21 21:57:41.561 | INFO     | Naive.node:maximize_level_node:228 - New level for node: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create-tree phase: start node splitting\n",
      "Create-tree phase: finish node splitting\n"
     ]
    }
   ],
   "source": [
    "# ********* create-tree phase **********\n",
    "good_leaf_nodes = list()\n",
    "bad_leaf_nodes = list()\n",
    "max_level=4\n",
    "p_value=2\n",
    "paa_value=2\n",
    "\n",
    "    \n",
    "print(\"Create-tree phase: start node splitting\")\n",
    "node = Node(level=1, group=time_series_dict, paa_value=2)\n",
    "node.start_splitting(p_value, max_level, good_leaf_nodes, bad_leaf_nodes)\n",
    "print(\"Create-tree phase: finish node splitting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc4244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start recycle bad-leaves phase\n",
      "Finish recycle bad-leaves phase\n"
     ]
    }
   ],
   "source": [
    "# ********** Recycle bad-leaves phase ********** \n",
    "    \n",
    "def recycle_bad_leaf(p_value, good_leaf_nodes, bad_leaf_nodes, suppressed_nodes, paa_value):\n",
    "    \n",
    "    bad_leaf_nodes_dict = dict()\n",
    "    \n",
    "    for node in bad_leaf_nodes:\n",
    "        if node.level in bad_leaf_nodes_dict.keys():\n",
    "            bad_leaf_nodes_dict[node.level].append(node)\n",
    "        else:\n",
    "            bad_leaf_nodes_dict[node.level] = [node]\n",
    "\n",
    "    bad_leaf_nodes_size = sum([node.size for node in bad_leaf_nodes])\n",
    "        \n",
    "    if bad_leaf_nodes_size >= p_value:\n",
    "        \n",
    "        current_level = max(bad_leaf_nodes_dict.keys())\n",
    "        \n",
    "        while bad_leaf_nodes_size >= p_value:\n",
    "            \n",
    "            if current_level in bad_leaf_nodes_dict.keys():\n",
    "                merge_dict = dict()\n",
    "                keys_to_be_removed = list()\n",
    "                merge = False\n",
    "                \n",
    "                for current_level_node in bad_leaf_nodes_dict[current_level]:\n",
    "                    pr_node = current_level_node.pattern_representation\n",
    "                    if pr_node in merge_dict.keys():\n",
    "                        merge = True\n",
    "                        merge_dict[pr_node].append(current_level_node)\n",
    "                        if pr_node in keys_to_be_removed:\n",
    "                            keys_to_be_removed.remove(pr_node)\n",
    "                        else:\n",
    "                            merge_dict[pr_node] = [current_level_node]\n",
    "                            keys_to_be_removed.append(pr_node)\n",
    "                    \n",
    "                    if merge:\n",
    "                        for k in keys_to_be_removed:\n",
    "                            del merge_dict[k]\n",
    "\n",
    "                        for pr, node_list in merge_dict.items():\n",
    "                            group = dict()\n",
    "                            for node in node_list:\n",
    "                                bad_leaf_nodes_dict[current_level].remove(node)\n",
    "                                group.update(node.group)\n",
    "                            if current_level > 1:\n",
    "                                level = current_level\n",
    "                            else:\n",
    "                                level = 1\n",
    "                            leaf_merge = Node(level=level, pattern_representation=pr,\n",
    "                                group=group, paa_value=paa_value)\n",
    "\n",
    "                            if leaf_merge.size >= p_value:\n",
    "                                leaf_merge.label = \"good-leaf\"\n",
    "                                good_leaf_nodes.append(leaf_merge)\n",
    "                                bad_leaf_nodes_size -= leaf_merge.size\n",
    "                            else: \n",
    "                                leaf_merge.label = \"bad-leaf\"\n",
    "                                bad_leaf_nodes_dict[current_level].append(leaf_merge)\n",
    "\n",
    "                temp_level = current_level-1\n",
    "                for node in bad_leaf_nodes_dict[current_level]:\n",
    "                    if temp_level > 1:\n",
    "                        values_group = list(node.group.values())\n",
    "                        data = np.array(values_group[0])\n",
    "                        data_znorm = znorm(data)\n",
    "                        data_paa = paa(data_znorm, paa_value)\n",
    "                        pr = ts_to_string(data_paa, cuts_for_asize(temp_level))\n",
    "                    else:\n",
    "                        pr = \"a\"*paa_value\n",
    "                    node.level = temp_level\n",
    "                    node.pattern_representation = pr\n",
    "\n",
    "                if current_level > 0:\n",
    "                    if temp_level not in bad_leaf_nodes_dict.keys():\n",
    "                        bad_leaf_nodes_dict[temp_level] = bad_leaf_nodes_dict.pop(current_level)\n",
    "                    else:\n",
    "                        bad_leaf_nodes_dict[temp_level] = bad_leaf_nodes_dict[temp_level] + bad_leaf_nodes_dict.pop(current_level) \n",
    "                    current_level -= 1\n",
    "                else:\n",
    "                    break \n",
    "\n",
    "        \n",
    "        remaining_bad_leaf_nodes = list(bad_leaf_nodes_dict.values())[0]\n",
    "        for node in remaining_bad_leaf_nodes:\n",
    "            suppressed_nodes.append(node)\n",
    "\n",
    "print(\"Start recycle bad-leaves phase\")\n",
    "suppressed_nodes = list()\n",
    "if(len(bad_leaf_nodes) > 0):\n",
    "    recycle_bad_leaf(p_value, good_leaf_nodes, bad_leaf_nodes, suppressed_nodes, paa_value)\n",
    "print(\"Finish recycle bad-leaves phase\")\n",
    "suppressed_nodes_list = list()\n",
    "        \n",
    "for node in suppressed_nodes:\n",
    "    suppressed_nodes_list.append(node.group) \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e602a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** Group Formation Phase ********\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def top_down_greedy_clustering(T, size, T_clustered,T_structure, label='o', T_max_vals=None, T_min_vals=None):\n",
    "    \n",
    "    if len(T) < 2*size:\n",
    "        T_clustered.append(T)\n",
    "        T_structure.append(label)\n",
    "        return\n",
    "\n",
    "    ids = list(T.keys())\n",
    "\n",
    "    # 1. Initialize groups via a NCP maximization-based heuristic\n",
    "    group_u = dict()\n",
    "    group_v = dict()\n",
    "\n",
    "    seed = ids[random.randint(0, len(ids) - 1)] # Draw a random row Id\n",
    "    group_u[seed] = T[seed]\n",
    "\n",
    "    old = seed # Last visited record\n",
    "\n",
    "    # to avoid this row to end up in two different groups\n",
    "    del T[seed]\n",
    "    ids.remove(seed)\n",
    "\n",
    "\n",
    "    rounds = 6 if len(T) >= 6 else len(T)\n",
    "\n",
    "    for rnd in range(rounds):\n",
    "        if rnd % 2 == 0:\n",
    "            source = group_u\n",
    "            target = group_v\n",
    "        else:\n",
    "            source = group_v\n",
    "            target = group_u\n",
    "            \n",
    "        r = find_tuple_with_max_vl(source[old], T, old)\n",
    "\n",
    "        target[r] = T[r]\n",
    "        old = r\n",
    "\n",
    "        # Update data structures\n",
    "        del T[r]\n",
    "        ids.remove(r)\n",
    "\n",
    "    # 1.b Assign each record to the group with lower NCP\n",
    "    random.shuffle(ids) # Shuffle leftover Ids\n",
    "\n",
    "    for i in ids:\n",
    "        row = T[i]\n",
    "\n",
    "        # Copy values to check what would happen\n",
    "        # if row was added to either one separately\n",
    "        group_u_vals = list(group_u.values())\n",
    "        group_v_vals = list(group_v.values())\n",
    "\n",
    "        group_u_vals.append(row)\n",
    "        group_v_vals.append(row)\n",
    "        \n",
    "        metric_u = instant_value_loss(group_u_vals)\n",
    "        metric_v = instant_value_loss(group_v_vals)\n",
    "\n",
    "        if metric_v < metric_u:\n",
    "            group_v[i] = row\n",
    "            del group_u_vals[-1]\n",
    "        else:\n",
    "            group_u[i] = row\n",
    "            del group_v_vals[-1]\n",
    "\n",
    "        del T[i]\n",
    "\n",
    "    # 2. Iterate recursively, or store groups if base case\n",
    "    if len(group_u) >= size:\n",
    "        top_down_greedy_clustering(group_u, size, T_clustered, T_structure, label + 'a', T_max_vals, T_min_vals) \n",
    "        T_clustered.append(group_u)\n",
    "        T_structure.append(label + 'a')\n",
    "\n",
    "    if len(group_v) >= size:\n",
    "        top_down_greedy_clustering(group_v, size, T_clustered, T_structure, label + 'b', T_max_vals, T_min_vals) \n",
    "    else:\n",
    "        T_clustered.append(group_v)\n",
    "        T_structure.append(label + 'b')\n",
    "\n",
    "\n",
    "def find_tuple_with_max_vl(base, T, key):\n",
    "    max_vl = 0\n",
    "    best = None\n",
    "\n",
    "    for k in T.keys():\n",
    "        if k != key:\n",
    "            vl = instant_value_loss([base, T[k]])\n",
    "\n",
    "            if vl >= max_vl: # Update the best tuple Id\n",
    "                max_vl = vl\n",
    "                best = k\n",
    "\n",
    "    return best\n",
    "\n",
    "def instant_value_loss(T, r_plus=None, r_minus=None):\n",
    "  \n",
    "\n",
    "    n = len(T[0])  # # of QI attributes in T\n",
    "\n",
    "    if not r_plus or not r_minus:\n",
    "        r_plus  = list()\n",
    "        r_minus = list()\n",
    "\n",
    "\n",
    "        for i in range(n): \n",
    "            r_plus_i  = 0\n",
    "            r_minus_i = float('inf')\n",
    "\n",
    "            for row in T:\n",
    "                if row[i] > r_plus_i:\n",
    "                    r_plus_i = row[i]\n",
    "\n",
    "                if row[i] < r_minus_i:\n",
    "                    r_minus_i = row[i]\n",
    "\n",
    "            r_plus.append(r_plus_i) \n",
    "            r_minus.append(r_minus_i)\n",
    "    \n",
    "    # Compute VL(t) and then VL(T)\n",
    "    vl_t = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        vl_t += pow((r_plus[i] - r_minus[i]), 2) / n\n",
    "\n",
    "    vl_T = len(T)*np.sqrt(vl_t)\n",
    "    return vl_T\n",
    "\n",
    "\n",
    "def top_down_greedy_clustering_postprocessing(size, T_clustered, T_structure,T_postprocessed, T_max_vals=None, \n",
    "                                              T_min_vals=None):\n",
    "    idxs_merged = list()      \n",
    "    groups_merged = list()    \n",
    "    structure_merged = list() \n",
    "\n",
    "  \n",
    "    for idx, bad_group in enumerate(T_clustered):\n",
    "        bad_g_size = len(bad_group)\n",
    "        if bad_g_size < size: # For any bad group\n",
    "            bad_group_vals = list(bad_group.values())\n",
    "           \n",
    "            label = T_structure[idx]\n",
    "\n",
    "            \n",
    "            idx_nn = -1\n",
    "            found_nn = False\n",
    "            metric_nn = float('inf')\n",
    "\n",
    "           \n",
    "            for other_idx, other_label in enumerate(T_structure):\n",
    "                \n",
    "                if label[:-1] == other_label[:-1]: \n",
    "                    if idx == other_idx:\n",
    "                        continue\n",
    "\n",
    "                   \n",
    "                    if other_idx not in idxs_merged:\n",
    "                        found_nn = True\n",
    "                        idx_nn = other_idx\n",
    "                        break\n",
    "           \n",
    "\n",
    "            merge_with_other_group = False\n",
    "            if found_nn:\n",
    "                group_nn = T_clustered[idx_nn]\n",
    "            elif idx_nn !=idx:\n",
    "                if idx - 1 > 0:\n",
    "                    idx_nn = idx - 1\n",
    "                elif idx + 1 < len(T_structure) - 1:\n",
    "                    idx_nn = idx + 1 \n",
    "                group_nn = T_clustered[idx_nn]\n",
    "                merge_with_other_group = True\n",
    "\n",
    "            if found_nn or merge_with_other_group:\n",
    "                group_merged_nn = bad_group_vals\n",
    "\n",
    "                \n",
    "                group_merged_nn = group_merged_nn  + list(group_nn.values())\n",
    "                \n",
    "                metric_nn = instant_value_loss(group_merged_nn)\n",
    "\n",
    "                # Redefine group_merged_nn as dict\n",
    "                group_merged_nn = dict()\n",
    "                group_merged_nn.update(bad_group)\n",
    "                group_merged_nn.update(group_nn)\n",
    "\n",
    "            # Find the most appropriate large group (>= 2*size -|G|) - 2nd candidate group\n",
    "            metric_large_g = float('inf')\n",
    "            idx_large_g = -1\n",
    "\n",
    "            for other_idx, other_group in enumerate(T_clustered):\n",
    "               \n",
    "                if len(other_group) >= 2*size - bad_g_size: \n",
    "                   \n",
    "                    if other_idx not in idxs_merged:\n",
    "                        group_merged_large_g = bad_group.copy()\n",
    "                        group_large_g_vals = list(group_merged_large_g.values())\n",
    "\n",
    "                       \n",
    "                        for j in range(size - bad_g_size): # size - |G|\n",
    "                            tmp_metric = float('inf')\n",
    "\n",
    "                            best_record = {}\n",
    "                            best_row = []\n",
    "\n",
    "                           \n",
    "                            for ridx, row in other_group.items():\n",
    "                                if ridx not in group_merged_large_g.keys():\n",
    "                                    \n",
    "                                    metric = instant_value_loss(group_large_g_vals + [ row ])\n",
    "\n",
    "                                    if metric < tmp_metric: # Update min metric\n",
    "                                        best_record = { ridx : row }\n",
    "                                        tmp_metric = metric\n",
    "                                        best_row = row\n",
    "            \n",
    "                            group_merged_large_g.update(best_record)\n",
    "                            group_large_g_vals.append(best_row)\n",
    "\n",
    "                      \n",
    "                        if tmp_metric < metric_large_g:\n",
    "                            metric_large_g = tmp_metric\n",
    "                            idx_large_g = other_idx\n",
    "\n",
    "                          \n",
    "                            leftover_group_large_g = { k : val for (k, val)\n",
    "                                    in other_group.items()\n",
    "                                    if k not in group_merged_large_g.keys() }\n",
    "     \n",
    "            if metric_nn < metric_large_g: \n",
    "                idxs_merged.append(idx_nn)\n",
    "                groups_merged.append(group_merged_nn)\n",
    "                structure_merged.append(label[:-1]) \n",
    "               \n",
    "            else:\n",
    "               \n",
    "                idxs_merged.append(idx_large_g)\n",
    "                groups_merged.append(group_merged_large_g)\n",
    "                groups_merged.append(leftover_group_large_g)\n",
    "              \n",
    "                structure_merged.append('')\n",
    "\n",
    "           \n",
    "            idxs_merged.append(idx)\n",
    "\n",
    "   \n",
    "    T_clustered = [ group for (idx, group)\n",
    "            in enumerate(T_clustered)\n",
    "            if idx not in idxs_merged ]\n",
    "    T_clustered += groups_merged \n",
    "\n",
    "    T_structure = [ label for (idx, label)\n",
    "            in enumerate(T_structure)\n",
    "            if idx not in idxs_merged]\n",
    "    T_structure += structure_merged\n",
    "\n",
    "    T_postprocessed += T_clustered\n",
    "\n",
    "    # 3. Check if there are any more bad groups\n",
    "    bad_groups_cnt = 0\n",
    "\n",
    "    for group in T_clustered:\n",
    "        if len(group) < size:\n",
    "            bad_groups_cnt +=1\n",
    "\n",
    "    if bad_groups_cnt > 0: # Call recursively if any left\n",
    "        top_down_greedy_clustering_postprocessing(size, T_clustered, T_structure,T_postprocessed, T_max_vals, T_min_vals)\n",
    "        \n",
    "\n",
    "def find_group_with_min_vl(group_to_search=None, group_to_merge=dict(), index_ignored=list()):\n",
    "    min_p_group = {\"group\" : dict(), \"index\" : None, \"vl\" : float(\"inf\")} \n",
    "    for index, group in enumerate(group_to_search):\n",
    "        if index not in index_ignored: \n",
    "            vl = instant_value_loss(list(group.values()) + list(group_to_merge.values()))\n",
    "            if vl < min_p_group[\"vl\"]:\n",
    "                min_p_group[\"vl\"] = vl\n",
    "                min_p_group[\"group\"] = group\n",
    "                min_p_group[\"index\"] = index\n",
    "\n",
    "    return min_p_group[\"group\"], min_p_group[\"index\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66cba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start group formation phase\n",
      "Finish group formation phase\n"
     ]
    }
   ],
   "source": [
    "k_value = 4\n",
    "print(\"Start group formation phase\")\n",
    "pattern_representation_dict = dict() \n",
    "p_group_list = list() \n",
    "time_series_postprocessed = list()\n",
    "\n",
    "for node in good_leaf_nodes:\n",
    "    p_group_list.append(node.group)\n",
    "    pr = node.pattern_representation\n",
    "    \n",
    "    for key in node.group:\n",
    "        pattern_representation_dict[key] = pr\n",
    "\n",
    "p_group_to_add = list()\n",
    "index_to_remove = list()\n",
    "\n",
    "for index, p_group in enumerate(p_group_list): \n",
    "    \n",
    "    if len(p_group) >= 2*p_value:\n",
    "        \n",
    "        tree_structure = list()\n",
    "        p_group_splitted = list()\n",
    "        p_group_to_split = p_group \n",
    "\n",
    "        # start top down greedy clustering\n",
    "        print('start')\n",
    "        top_down_greedy_clustering(p_group_to_split, p_value,p_group_splitted, tree_structure)\n",
    "        print('END')\n",
    "\n",
    "       \n",
    "        top_down_greedy_clustering_postprocessing(p_value,p_group_splitted, tree_structure ,time_series_postprocessed)\n",
    "                                                  \n",
    "            \n",
    "                \n",
    "    p_group_to_add += time_series_postprocessed\n",
    "    index_to_remove.append(index)\n",
    "        \n",
    "        \n",
    "p_group_list = [group for (index, group) in enumerate(p_group_list) if index not in index_to_remove ]\n",
    "p_group_list += p_group_to_add\n",
    "        \n",
    "        \n",
    "k_group_list = list()\n",
    "index_to_remove = list() \n",
    "        \n",
    "# step 1\n",
    "for index, group in enumerate(p_group_list):\n",
    "    if len(group) >= k_value:\n",
    "        index_to_remove.append(index)\n",
    "        k_group_list.append(group)\n",
    "        \n",
    "p_group_list = [group for (index, group) in enumerate(p_group_list) if index not in index_to_remove ]\n",
    "\n",
    "# Remove all P groups from PG list \n",
    "index_to_remove = list()\n",
    "p_group_list_size = sum([len(group) for group in p_group_list])\n",
    "        \n",
    "while p_group_list_size >= k_value:\n",
    "    k_group, index_min = find_group_with_min_vl(group_to_search=p_group_list, \n",
    "                                                                index_ignored=index_to_remove)\n",
    "    index_to_remove.append(index_min)\n",
    "    p_group_list_size -= len(k_group)\n",
    "\n",
    "    while len(k_group) < k_value:\n",
    "        group_to_add, index_group_to_add = find_group_with_min_vl(group_to_search=p_group_list,\n",
    "                                                                                  group_to_merge=k_group, \n",
    "                                                                                  index_ignored=index_to_remove)\n",
    "        index_to_remove.append(index_group_to_add)\n",
    "        k_group.update(group_to_add) \n",
    "        p_group_list_size -= len(group_to_add)\n",
    "    k_group_list.append(k_group)   \n",
    "    \n",
    "        \n",
    "# For remaining P-groups, finding corresponding G and add to subgroup list\n",
    "p_group_remaining = [group for (index, group) in enumerate(p_group_list) if index not in index_to_remove ]\n",
    "        \n",
    "for p_group in p_group_remaining:\n",
    "    k_group, index_k_group = find_group_with_min_vl(group_to_search=k_group_list,\n",
    "                                                                    group_to_merge=p_group)\n",
    "    k_group_list.pop(index_k_group)\n",
    "    k_group.update(p_group)\n",
    "    k_group_list.append(k_group)\n",
    "\n",
    "print(\"Finish group formation phase\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5286a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def compute_anonymized_data(pattern_anonymized_data,anonymized_data,suppressed_data):\n",
    "    \n",
    "    final_data_anonymized= dict()\n",
    "    \n",
    "    for index in range(0, len(anonymized_data)): \n",
    "        \n",
    "        group = anonymized_data[index]\n",
    "        \n",
    "        max_value = np.amax(np.array(list(group.values())), 0)\n",
    "        min_value = np.amin(np.array(list(group.values())), 0)\n",
    "        \n",
    "        for key in group.keys():\n",
    "            \n",
    "            final_data_anonymized[key] = list()\n",
    "            value_row = list()\n",
    "            for column_index in range(0, len(max_value)):\n",
    "                value_row.append(\"[{}-{}]\".format(min_value[column_index], max_value[column_index]))\n",
    "            \n",
    "            \n",
    "            value_row.append(pattern_anonymized_data[key]) \n",
    "            value_row.append(\"Group: {}\".format(index))\n",
    "\n",
    "            final_data_anonymized[key] = value_row\n",
    "           \n",
    "        \n",
    "    for index in range(0, len(suppressed_data)):\n",
    "        group = suppressed_data[index]\n",
    "        for key in group.keys():\n",
    "            value_row = [\" - \"]*len(group[key])\n",
    "            value_row.append(\" - \") # pattern rapresentation\n",
    "            value_row.append(\" - \") # group\n",
    "            final_data_anonymized[key] = value_row\n",
    "            \n",
    "    \n",
    "    your_df_from_dict=pd.DataFrame.from_dict(final_data_anonymized,orient='index')\n",
    "    print(your_df_from_dict)\n",
    "            \n",
    "compute_anonymized_data(pattern_representation_dict,k_group_list,suppressed_nodes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687dd191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
